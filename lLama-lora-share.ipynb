{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cc41ad3-b6f2-453c-bbd5-9436768b577a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformers: 4.56.2\n",
      "peft: 0.7.1\n",
      "datasets: 4.1.1\n",
      "torch: 2.5.1\n",
      "accelerate: 1.10.1\n",
      "WARNING:tensorflow:From C:\\Users\\Administrator\\.conda\\envs\\p311\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import transformers, peft, datasets, torch, accelerate\n",
    "\n",
    "print(\"transformers:\", transformers.__version__)\n",
    "print(\"peft:\", peft.__version__)\n",
    "print(\"datasets:\", datasets.__version__)\n",
    "print(\"torch:\", torch.__version__)\n",
    "print(\"accelerate:\", accelerate.__version__)\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "121d53c2-fe06-4aef-907c-2fa2dc3bea9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Administrator\\.conda\\envs\\p311\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Train: 6406, Val: 800, Test: 801\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fda97b997c6417883a04d6aa357ef33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6406 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7d3834aa94f4e519a6937cce86d3354",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_2276\\107858964.py:102: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 2}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6406' max='6406' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6406/6406 45:48, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>11.954500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.964700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.859400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.521700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.369500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.337000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.307100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.335900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.351800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.291900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.326100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.186200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.302000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.278800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.396700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.219200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.209800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.251000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.279400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.232200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.276300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.318600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.188000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.269900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.180500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.207900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.219500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.353000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.240800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.200600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.437900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.264500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.505400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.364100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.281100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.205300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.212500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.307500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.187100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.211100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>0.386800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.200300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>0.335500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.358500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.211400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.229400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>0.325300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.376100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>0.272700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.329100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>0.236700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.306100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>0.295600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.338900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.346000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.183200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>0.331800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>0.324700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590</td>\n",
       "      <td>0.214600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.292800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>610</td>\n",
       "      <td>0.312100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>0.230500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>0.358700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.322300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.246600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>0.274100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>670</td>\n",
       "      <td>0.278200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>0.117300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>690</td>\n",
       "      <td>0.250900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.204500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>710</td>\n",
       "      <td>0.107300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.245800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>730</td>\n",
       "      <td>0.299400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>0.269200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.193400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>0.291800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>770</td>\n",
       "      <td>0.351700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>0.261200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>790</td>\n",
       "      <td>0.306600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.298100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>810</td>\n",
       "      <td>0.234000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>0.354700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>830</td>\n",
       "      <td>0.362100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>0.306900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.188800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>0.323800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>870</td>\n",
       "      <td>0.304800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.182000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>890</td>\n",
       "      <td>0.289700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.312100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>910</td>\n",
       "      <td>0.248000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>0.344200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>930</td>\n",
       "      <td>0.374600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>940</td>\n",
       "      <td>0.189000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.266100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.304100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>970</td>\n",
       "      <td>0.300600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>980</td>\n",
       "      <td>0.303700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>990</td>\n",
       "      <td>0.160000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.282000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1010</td>\n",
       "      <td>0.242300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1020</td>\n",
       "      <td>0.462600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1030</td>\n",
       "      <td>0.382200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>0.197000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.265100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1060</td>\n",
       "      <td>0.320200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1070</td>\n",
       "      <td>0.169800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1080</td>\n",
       "      <td>0.227800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1090</td>\n",
       "      <td>0.292300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.314500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1110</td>\n",
       "      <td>0.351700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>0.277600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1130</td>\n",
       "      <td>0.330100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1140</td>\n",
       "      <td>0.274000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.178900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1160</td>\n",
       "      <td>0.343200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1170</td>\n",
       "      <td>0.305500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1180</td>\n",
       "      <td>0.241900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1190</td>\n",
       "      <td>0.262800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.367600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1210</td>\n",
       "      <td>0.258500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1220</td>\n",
       "      <td>0.158700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1230</td>\n",
       "      <td>0.325300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1240</td>\n",
       "      <td>0.195100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.235500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1260</td>\n",
       "      <td>0.354600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1270</td>\n",
       "      <td>0.315000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>0.299000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1290</td>\n",
       "      <td>0.232700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.294400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1310</td>\n",
       "      <td>0.182500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1320</td>\n",
       "      <td>0.271200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1330</td>\n",
       "      <td>0.219100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1340</td>\n",
       "      <td>0.348900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.298900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>0.235100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1370</td>\n",
       "      <td>0.295200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1380</td>\n",
       "      <td>0.403000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1390</td>\n",
       "      <td>0.238400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.212200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1410</td>\n",
       "      <td>0.266800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1420</td>\n",
       "      <td>0.199700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1430</td>\n",
       "      <td>0.273000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>0.161300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.241800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1460</td>\n",
       "      <td>0.246000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1470</td>\n",
       "      <td>0.257000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1480</td>\n",
       "      <td>0.272300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1490</td>\n",
       "      <td>0.318900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.333300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1510</td>\n",
       "      <td>0.356000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1520</td>\n",
       "      <td>0.255900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1530</td>\n",
       "      <td>0.185100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1540</td>\n",
       "      <td>0.172000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.240100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1560</td>\n",
       "      <td>0.419200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1570</td>\n",
       "      <td>0.463500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1580</td>\n",
       "      <td>0.351100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1590</td>\n",
       "      <td>0.228600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.313900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1610</td>\n",
       "      <td>0.366300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1620</td>\n",
       "      <td>0.285900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1630</td>\n",
       "      <td>0.215600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1640</td>\n",
       "      <td>0.288200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.327100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1660</td>\n",
       "      <td>0.289800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1670</td>\n",
       "      <td>0.214600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1680</td>\n",
       "      <td>0.231600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1690</td>\n",
       "      <td>0.226000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.324500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1710</td>\n",
       "      <td>0.280700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1720</td>\n",
       "      <td>0.267100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1730</td>\n",
       "      <td>0.157000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1740</td>\n",
       "      <td>0.194800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.242000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1760</td>\n",
       "      <td>0.210300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1770</td>\n",
       "      <td>0.292000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1780</td>\n",
       "      <td>0.280800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1790</td>\n",
       "      <td>0.235100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.213000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1810</td>\n",
       "      <td>0.245200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1820</td>\n",
       "      <td>0.307200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1830</td>\n",
       "      <td>0.277800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1840</td>\n",
       "      <td>0.319600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>0.274100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1860</td>\n",
       "      <td>0.203800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1870</td>\n",
       "      <td>0.316700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1880</td>\n",
       "      <td>0.192300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1890</td>\n",
       "      <td>0.175600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.156400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1910</td>\n",
       "      <td>0.269000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1920</td>\n",
       "      <td>0.182000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1930</td>\n",
       "      <td>0.198200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1940</td>\n",
       "      <td>0.203000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>0.285900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1960</td>\n",
       "      <td>0.294400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1970</td>\n",
       "      <td>0.327800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1980</td>\n",
       "      <td>0.187000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1990</td>\n",
       "      <td>0.252700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.205300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2010</td>\n",
       "      <td>0.257500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020</td>\n",
       "      <td>0.271700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2030</td>\n",
       "      <td>0.241800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2040</td>\n",
       "      <td>0.170300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>0.191200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2060</td>\n",
       "      <td>0.226600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2070</td>\n",
       "      <td>0.184400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2080</td>\n",
       "      <td>0.231100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2090</td>\n",
       "      <td>0.188100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.354400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2110</td>\n",
       "      <td>0.248000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2120</td>\n",
       "      <td>0.291100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2130</td>\n",
       "      <td>0.216500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2140</td>\n",
       "      <td>0.288000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>0.207600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2160</td>\n",
       "      <td>0.135700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2170</td>\n",
       "      <td>0.273900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2180</td>\n",
       "      <td>0.263100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2190</td>\n",
       "      <td>0.296400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.203600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2210</td>\n",
       "      <td>0.246100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2220</td>\n",
       "      <td>0.331400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2230</td>\n",
       "      <td>0.297800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2240</td>\n",
       "      <td>0.228500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>0.228400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2260</td>\n",
       "      <td>0.229300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2270</td>\n",
       "      <td>0.216200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2280</td>\n",
       "      <td>0.231300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2290</td>\n",
       "      <td>0.273900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.229800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2310</td>\n",
       "      <td>0.314600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2320</td>\n",
       "      <td>0.205800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2330</td>\n",
       "      <td>0.267100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2340</td>\n",
       "      <td>0.158700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>0.264200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2360</td>\n",
       "      <td>0.288500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2370</td>\n",
       "      <td>0.393900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2380</td>\n",
       "      <td>0.249200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2390</td>\n",
       "      <td>0.232500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.331400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2410</td>\n",
       "      <td>0.325400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2420</td>\n",
       "      <td>0.197500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2430</td>\n",
       "      <td>0.249300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2440</td>\n",
       "      <td>0.211300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>0.247600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2460</td>\n",
       "      <td>0.179700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2470</td>\n",
       "      <td>0.291600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2480</td>\n",
       "      <td>0.242300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2490</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.272300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2510</td>\n",
       "      <td>0.188500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2520</td>\n",
       "      <td>0.215300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2530</td>\n",
       "      <td>0.271600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2540</td>\n",
       "      <td>0.284000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>0.352800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2560</td>\n",
       "      <td>0.210100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2570</td>\n",
       "      <td>0.268100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2580</td>\n",
       "      <td>0.300200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2590</td>\n",
       "      <td>0.256200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.193100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2610</td>\n",
       "      <td>0.247000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2620</td>\n",
       "      <td>0.295400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2630</td>\n",
       "      <td>0.294800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2640</td>\n",
       "      <td>0.199700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>0.227700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2660</td>\n",
       "      <td>0.518900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2670</td>\n",
       "      <td>0.202700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2680</td>\n",
       "      <td>0.224300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2690</td>\n",
       "      <td>0.288700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2710</td>\n",
       "      <td>0.342800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2720</td>\n",
       "      <td>0.240800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2730</td>\n",
       "      <td>0.389200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2740</td>\n",
       "      <td>0.257900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>0.327100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2760</td>\n",
       "      <td>0.157900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2770</td>\n",
       "      <td>0.158600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2780</td>\n",
       "      <td>0.203800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2790</td>\n",
       "      <td>0.325200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.184700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2810</td>\n",
       "      <td>0.315100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2820</td>\n",
       "      <td>0.303500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2830</td>\n",
       "      <td>0.237700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2840</td>\n",
       "      <td>0.249000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2850</td>\n",
       "      <td>0.152200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2860</td>\n",
       "      <td>0.322600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2870</td>\n",
       "      <td>0.351100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2880</td>\n",
       "      <td>0.130100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2890</td>\n",
       "      <td>0.379700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.345000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2910</td>\n",
       "      <td>0.223200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2920</td>\n",
       "      <td>0.258300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2930</td>\n",
       "      <td>0.275800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2940</td>\n",
       "      <td>0.209700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2950</td>\n",
       "      <td>0.200900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2960</td>\n",
       "      <td>0.330900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2970</td>\n",
       "      <td>0.384800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2980</td>\n",
       "      <td>0.267000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2990</td>\n",
       "      <td>0.280800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.364600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3010</td>\n",
       "      <td>0.240400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3020</td>\n",
       "      <td>0.371100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3030</td>\n",
       "      <td>0.170200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3040</td>\n",
       "      <td>0.261600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3050</td>\n",
       "      <td>0.257200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3060</td>\n",
       "      <td>0.212500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3070</td>\n",
       "      <td>0.284100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3080</td>\n",
       "      <td>0.199600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3090</td>\n",
       "      <td>0.273800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.212000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3110</td>\n",
       "      <td>0.198300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3120</td>\n",
       "      <td>0.289300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3130</td>\n",
       "      <td>0.192400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3140</td>\n",
       "      <td>0.291300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3150</td>\n",
       "      <td>0.261500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3160</td>\n",
       "      <td>0.239600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3170</td>\n",
       "      <td>0.134600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3180</td>\n",
       "      <td>0.283700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3190</td>\n",
       "      <td>0.239000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.174500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3210</td>\n",
       "      <td>0.193400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3220</td>\n",
       "      <td>0.203000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3230</td>\n",
       "      <td>0.158900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3240</td>\n",
       "      <td>0.283500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3250</td>\n",
       "      <td>0.284400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3260</td>\n",
       "      <td>0.182600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3270</td>\n",
       "      <td>0.268000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3280</td>\n",
       "      <td>0.232700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3290</td>\n",
       "      <td>0.212000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.372700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3310</td>\n",
       "      <td>0.286600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3320</td>\n",
       "      <td>0.257100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3330</td>\n",
       "      <td>0.255900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3340</td>\n",
       "      <td>0.472700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3350</td>\n",
       "      <td>0.270800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3360</td>\n",
       "      <td>0.247400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3370</td>\n",
       "      <td>0.383500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3380</td>\n",
       "      <td>0.271500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3390</td>\n",
       "      <td>0.272700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.185700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3410</td>\n",
       "      <td>0.212000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3420</td>\n",
       "      <td>0.289500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3430</td>\n",
       "      <td>0.188800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3440</td>\n",
       "      <td>0.410700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3450</td>\n",
       "      <td>0.152500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3460</td>\n",
       "      <td>0.298100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3470</td>\n",
       "      <td>0.172000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3480</td>\n",
       "      <td>0.274400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3490</td>\n",
       "      <td>0.317600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.239700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3510</td>\n",
       "      <td>0.332000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3520</td>\n",
       "      <td>0.305000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3530</td>\n",
       "      <td>0.227900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3540</td>\n",
       "      <td>0.197100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3550</td>\n",
       "      <td>0.233800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3560</td>\n",
       "      <td>0.203400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3570</td>\n",
       "      <td>0.262300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3580</td>\n",
       "      <td>0.165100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3590</td>\n",
       "      <td>0.402700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.264300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3610</td>\n",
       "      <td>0.319800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3620</td>\n",
       "      <td>0.321500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3630</td>\n",
       "      <td>0.184000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3640</td>\n",
       "      <td>0.196100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3650</td>\n",
       "      <td>0.241700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3660</td>\n",
       "      <td>0.210400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3670</td>\n",
       "      <td>0.202000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3680</td>\n",
       "      <td>0.159400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3690</td>\n",
       "      <td>0.191900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>0.247200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3710</td>\n",
       "      <td>0.170800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3720</td>\n",
       "      <td>0.199500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3730</td>\n",
       "      <td>0.186200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3740</td>\n",
       "      <td>0.251100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3750</td>\n",
       "      <td>0.227400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3760</td>\n",
       "      <td>0.208400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3770</td>\n",
       "      <td>0.231000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3780</td>\n",
       "      <td>0.450100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3790</td>\n",
       "      <td>0.246400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.178500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3810</td>\n",
       "      <td>0.230300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3820</td>\n",
       "      <td>0.240000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3830</td>\n",
       "      <td>0.265100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3840</td>\n",
       "      <td>0.213000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3850</td>\n",
       "      <td>0.209700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3860</td>\n",
       "      <td>0.247900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3870</td>\n",
       "      <td>0.172000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3880</td>\n",
       "      <td>0.312800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3890</td>\n",
       "      <td>0.233100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>0.205100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3910</td>\n",
       "      <td>0.251700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3920</td>\n",
       "      <td>0.231100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3930</td>\n",
       "      <td>0.156000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3940</td>\n",
       "      <td>0.243600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3950</td>\n",
       "      <td>0.228600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3960</td>\n",
       "      <td>0.174200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3970</td>\n",
       "      <td>0.341100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3980</td>\n",
       "      <td>0.184300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3990</td>\n",
       "      <td>0.235000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.289400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4010</td>\n",
       "      <td>0.258700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4020</td>\n",
       "      <td>0.202100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4030</td>\n",
       "      <td>0.237800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4040</td>\n",
       "      <td>0.229000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4050</td>\n",
       "      <td>0.336000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4060</td>\n",
       "      <td>0.273100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4070</td>\n",
       "      <td>0.341700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4080</td>\n",
       "      <td>0.158900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4090</td>\n",
       "      <td>0.265100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>0.253000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4110</td>\n",
       "      <td>0.238400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4120</td>\n",
       "      <td>0.322500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4130</td>\n",
       "      <td>0.328900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4140</td>\n",
       "      <td>0.260200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4150</td>\n",
       "      <td>0.280600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4160</td>\n",
       "      <td>0.195800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4170</td>\n",
       "      <td>0.313000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4180</td>\n",
       "      <td>0.309300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4190</td>\n",
       "      <td>0.291500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.236700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4210</td>\n",
       "      <td>0.161700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4220</td>\n",
       "      <td>0.241900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4230</td>\n",
       "      <td>0.170100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4240</td>\n",
       "      <td>0.146300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4250</td>\n",
       "      <td>0.353300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4260</td>\n",
       "      <td>0.340100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4270</td>\n",
       "      <td>0.213500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4280</td>\n",
       "      <td>0.265300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4290</td>\n",
       "      <td>0.339400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4310</td>\n",
       "      <td>0.251600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4320</td>\n",
       "      <td>0.182000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4330</td>\n",
       "      <td>0.252200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4340</td>\n",
       "      <td>0.231700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4350</td>\n",
       "      <td>0.265300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4360</td>\n",
       "      <td>0.223300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4370</td>\n",
       "      <td>0.245800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4380</td>\n",
       "      <td>0.259800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4390</td>\n",
       "      <td>0.342000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.140800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4410</td>\n",
       "      <td>0.200600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4420</td>\n",
       "      <td>0.252500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4430</td>\n",
       "      <td>0.316000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4440</td>\n",
       "      <td>0.216300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4450</td>\n",
       "      <td>0.317200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4460</td>\n",
       "      <td>0.291200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4470</td>\n",
       "      <td>0.187500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4480</td>\n",
       "      <td>0.196900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4490</td>\n",
       "      <td>0.262600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.276700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4510</td>\n",
       "      <td>0.260100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4520</td>\n",
       "      <td>0.140300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4530</td>\n",
       "      <td>0.350800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4540</td>\n",
       "      <td>0.142200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4550</td>\n",
       "      <td>0.404700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4560</td>\n",
       "      <td>0.152600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4570</td>\n",
       "      <td>0.244600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4580</td>\n",
       "      <td>0.175000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4590</td>\n",
       "      <td>0.258300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.268000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4610</td>\n",
       "      <td>0.351900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4620</td>\n",
       "      <td>0.189200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4630</td>\n",
       "      <td>0.223800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4640</td>\n",
       "      <td>0.402000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4650</td>\n",
       "      <td>0.410800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4660</td>\n",
       "      <td>0.321200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4670</td>\n",
       "      <td>0.134500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4680</td>\n",
       "      <td>0.177500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4690</td>\n",
       "      <td>0.163100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>0.317400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4710</td>\n",
       "      <td>0.311600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4720</td>\n",
       "      <td>0.300200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4730</td>\n",
       "      <td>0.218800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4740</td>\n",
       "      <td>0.282200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4750</td>\n",
       "      <td>0.205100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4760</td>\n",
       "      <td>0.323500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4770</td>\n",
       "      <td>0.265300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4780</td>\n",
       "      <td>0.186500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4790</td>\n",
       "      <td>0.349800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.254200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4810</td>\n",
       "      <td>0.186600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4820</td>\n",
       "      <td>0.208300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4830</td>\n",
       "      <td>0.155500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4840</td>\n",
       "      <td>0.277700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4850</td>\n",
       "      <td>0.300700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4860</td>\n",
       "      <td>0.165900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4870</td>\n",
       "      <td>0.162500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4880</td>\n",
       "      <td>0.196300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4890</td>\n",
       "      <td>0.285800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>0.294600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4910</td>\n",
       "      <td>0.342500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4920</td>\n",
       "      <td>0.237900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4930</td>\n",
       "      <td>0.173500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4940</td>\n",
       "      <td>0.159200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4950</td>\n",
       "      <td>0.252000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4960</td>\n",
       "      <td>0.255300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4970</td>\n",
       "      <td>0.213000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4980</td>\n",
       "      <td>0.199400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4990</td>\n",
       "      <td>0.187500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.273000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5010</td>\n",
       "      <td>0.396300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5020</td>\n",
       "      <td>0.220800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5030</td>\n",
       "      <td>0.282800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5040</td>\n",
       "      <td>0.244200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5050</td>\n",
       "      <td>0.250200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5060</td>\n",
       "      <td>0.215200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5070</td>\n",
       "      <td>0.201900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5080</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5090</td>\n",
       "      <td>0.278900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>0.333700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5110</td>\n",
       "      <td>0.177900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5120</td>\n",
       "      <td>0.206500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5130</td>\n",
       "      <td>0.218500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5140</td>\n",
       "      <td>0.190700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5150</td>\n",
       "      <td>0.179400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5160</td>\n",
       "      <td>0.142100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5170</td>\n",
       "      <td>0.231700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5180</td>\n",
       "      <td>0.224200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5190</td>\n",
       "      <td>0.244100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>0.363800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5210</td>\n",
       "      <td>0.178600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5220</td>\n",
       "      <td>0.258100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5230</td>\n",
       "      <td>0.195600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5240</td>\n",
       "      <td>0.297300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5250</td>\n",
       "      <td>0.280300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5260</td>\n",
       "      <td>0.208000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5270</td>\n",
       "      <td>0.185900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5280</td>\n",
       "      <td>0.204000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5290</td>\n",
       "      <td>0.343300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>0.258300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5310</td>\n",
       "      <td>0.137300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5320</td>\n",
       "      <td>0.284300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5330</td>\n",
       "      <td>0.292300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5340</td>\n",
       "      <td>0.211700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5350</td>\n",
       "      <td>0.260000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5360</td>\n",
       "      <td>0.174600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5370</td>\n",
       "      <td>0.275300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5380</td>\n",
       "      <td>0.274500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5390</td>\n",
       "      <td>0.276800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>0.177700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5410</td>\n",
       "      <td>0.272200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5420</td>\n",
       "      <td>0.184800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5430</td>\n",
       "      <td>0.212300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5440</td>\n",
       "      <td>0.287700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5450</td>\n",
       "      <td>0.157300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5460</td>\n",
       "      <td>0.315700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5470</td>\n",
       "      <td>0.461800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5480</td>\n",
       "      <td>0.334700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5490</td>\n",
       "      <td>0.224400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.297000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5510</td>\n",
       "      <td>0.212100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5520</td>\n",
       "      <td>0.228000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5530</td>\n",
       "      <td>0.224900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5540</td>\n",
       "      <td>0.273000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5550</td>\n",
       "      <td>0.196300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5560</td>\n",
       "      <td>0.163200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5570</td>\n",
       "      <td>0.211100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5580</td>\n",
       "      <td>0.222800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5590</td>\n",
       "      <td>0.178800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>0.322200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5610</td>\n",
       "      <td>0.378100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5620</td>\n",
       "      <td>0.327100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5630</td>\n",
       "      <td>0.316600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5640</td>\n",
       "      <td>0.209000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5650</td>\n",
       "      <td>0.575300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5660</td>\n",
       "      <td>0.183300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5670</td>\n",
       "      <td>0.197900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5680</td>\n",
       "      <td>0.220100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5690</td>\n",
       "      <td>0.303300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>0.217400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5710</td>\n",
       "      <td>0.234200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5720</td>\n",
       "      <td>0.180300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5730</td>\n",
       "      <td>0.258700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5740</td>\n",
       "      <td>0.235400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5750</td>\n",
       "      <td>0.179000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5760</td>\n",
       "      <td>0.178400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5770</td>\n",
       "      <td>0.146300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5780</td>\n",
       "      <td>0.312000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5790</td>\n",
       "      <td>0.270500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>0.235000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5810</td>\n",
       "      <td>0.168100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5820</td>\n",
       "      <td>0.176000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5830</td>\n",
       "      <td>0.154700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5840</td>\n",
       "      <td>0.146600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5850</td>\n",
       "      <td>0.205200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5860</td>\n",
       "      <td>0.126500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5870</td>\n",
       "      <td>0.146400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5880</td>\n",
       "      <td>0.381200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5890</td>\n",
       "      <td>0.209300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5900</td>\n",
       "      <td>0.208300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5910</td>\n",
       "      <td>0.260900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5920</td>\n",
       "      <td>0.160800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5930</td>\n",
       "      <td>0.213500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5940</td>\n",
       "      <td>0.184300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5950</td>\n",
       "      <td>0.172000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5960</td>\n",
       "      <td>0.210800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5970</td>\n",
       "      <td>0.191400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5980</td>\n",
       "      <td>0.254600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5990</td>\n",
       "      <td>0.155100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.282600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6010</td>\n",
       "      <td>0.326600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6020</td>\n",
       "      <td>0.248500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6030</td>\n",
       "      <td>0.325200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6040</td>\n",
       "      <td>0.235100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6050</td>\n",
       "      <td>0.191900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6060</td>\n",
       "      <td>0.202800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6070</td>\n",
       "      <td>0.364200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6080</td>\n",
       "      <td>0.223900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6090</td>\n",
       "      <td>0.174000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6100</td>\n",
       "      <td>0.234400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6110</td>\n",
       "      <td>0.146100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6120</td>\n",
       "      <td>0.315900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6130</td>\n",
       "      <td>0.206200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6140</td>\n",
       "      <td>0.202600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6150</td>\n",
       "      <td>0.152300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6160</td>\n",
       "      <td>0.192900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6170</td>\n",
       "      <td>0.213300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6180</td>\n",
       "      <td>0.178200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6190</td>\n",
       "      <td>0.175400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>0.205000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6210</td>\n",
       "      <td>0.324000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6220</td>\n",
       "      <td>0.313200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6230</td>\n",
       "      <td>0.233200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6240</td>\n",
       "      <td>0.173600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6250</td>\n",
       "      <td>0.232800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6260</td>\n",
       "      <td>0.304200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6270</td>\n",
       "      <td>0.299500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6280</td>\n",
       "      <td>0.240600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6290</td>\n",
       "      <td>0.232200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6300</td>\n",
       "      <td>0.211300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6310</td>\n",
       "      <td>0.219000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6320</td>\n",
       "      <td>0.279800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6330</td>\n",
       "      <td>0.194200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6340</td>\n",
       "      <td>0.248100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6350</td>\n",
       "      <td>0.239600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6360</td>\n",
       "      <td>0.291300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6370</td>\n",
       "      <td>0.375800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6380</td>\n",
       "      <td>0.217800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6390</td>\n",
       "      <td>0.305500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>0.194300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#train the gpt-based model (llama-lora) and save it\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# -----------------------------\n",
    "# 1 Config\n",
    "# --------------- \n",
    "CSV_PATH   = \"001_2802_merged_12000.csv\"\n",
    "TEXT_COL   = \"text\"\n",
    "LABEL_COL  = \"label\"\n",
    "MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "OUTPUT_DIR = \"./llama_lora_clean\"\n",
    "EPOCHS     = 2   # train a bit longer\n",
    "BATCH_SIZE = 2\n",
    "\n",
    "# -----------------------------\n",
    "# 2 Prompt template\n",
    "# ----------------------- \n",
    "PROMPT_TEMPLATE = (\n",
    "    \"You are a clinical assistant. Classify the text for depression risk.\\n\"\n",
    "    \"Return ONLY a single digit: 1 if depressed, 0 if not depressed.\\n\"\n",
    "    \"Do not write any other words.\\n\\n\"\n",
    "    \"Text: {text}\\n\\n\"\n",
    "    \"Answer:\"\n",
    ")\n",
    "\n",
    "def build_prompt(text, label):\n",
    "    return PROMPT_TEMPLATE.format(text=text.strip()) + f\" {int(label)}\"\n",
    "\n",
    "# -----------------------------\n",
    "# Load and split dataset\n",
    "# ---------------- \n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "train_val_df, test_df = train_test_split(\n",
    "    df, test_size=0.1, random_state=42, stratify=df[LABEL_COL]\n",
    ")\n",
    "train_df, val_df = train_test_split(\n",
    "    train_val_df, test_size=0.111, random_state=42, stratify=train_val_df[LABEL_COL]\n",
    ")\n",
    "\n",
    "print(f\"Train: {len(train_df)}, Val: {len(val_df)}, Test: {len(test_df)}\")\n",
    "test_df.to_csv(\"test_split.csv\", index=False)\n",
    "\n",
    "train_df[\"train_text\"] = train_df.apply(lambda r: build_prompt(r[TEXT_COL], r[LABEL_COL]), axis=1)\n",
    "val_df[\"train_text\"]   = val_df.apply(lambda r: build_prompt(r[TEXT_COL], r[LABEL_COL]), axis=1)\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df[[\"train_text\"]])\n",
    "val_dataset   = Dataset.from_pandas(val_df[[\"train_text\"]])\n",
    "\n",
    "# -----------------------------\n",
    "# 3 Tokenizer\n",
    "# -----------------------------\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def preprocess(batch):\n",
    "    enc = tokenizer(\n",
    "        batch[\"train_text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=256\n",
    "    )\n",
    "    enc[\"labels\"] = enc[\"input_ids\"].copy()\n",
    "    return enc\n",
    "\n",
    "train_dataset = train_dataset.map(preprocess, batched=True, remove_columns=[\"train_text\"])\n",
    "val_dataset   = val_dataset.map(preprocess, batched=True, remove_columns=[\"train_text\"])\n",
    "\n",
    "# -----------------------------\n",
    "# 4 Model + LoRA\n",
    "# --------------------- \n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.1,\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "# -----------------------------\n",
    "# 5 Training\n",
    "# --------------------- \n",
    "args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-4,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(OUTPUT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4e685c2-94cb-4387-9077-9f3c24aebae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.6579275905118602\n",
      "Precision: 0.5991561181434599\n",
      "Recall   : 0.7717391304347826\n",
      "F1       : 0.6745843230403801\n",
      "AUC      : 0.6664700271111557\n",
      "Confusion matrix:\n",
      " [[243 190]\n",
      " [ 84 284]]\n"
     ]
    }
   ],
   "source": [
    "#Test the saved model\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score, confusion_matrix\n",
    "\n",
    "# -----------------------------\n",
    "# 1 Config\n",
    "# ------------------- \n",
    "MODEL_PATH = \"./llama_lora_clean\"\n",
    "TEST_PATH  = \"test_split.csv\"\n",
    "TEXT_COL   = \"text\"\n",
    "LABEL_COL  = \"label\"\n",
    "\n",
    "PROMPT_TEMPLATE = (\n",
    "    \"You are a clinical assistant. Classify the text for depression risk.\\n\"\n",
    "    \"Return ONLY a single digit: 1 if depressed, 0 if not depressed.\\n\"\n",
    "    \"Do not write any other words.\\n\\n\"\n",
    "    \"Text: {text}\\n\\n\"\n",
    "    \"Answer:\"\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 2 Load model + tokenizer\n",
    "# ------------------- \n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "model     = AutoModelForCausalLM.from_pretrained(MODEL_PATH)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "# ------------------ \n",
    "# 3 Load test set\n",
    "# --------------- \n",
    "df_test = pd.read_csv(TEST_PATH)\n",
    "\n",
    "# -----------------------------\n",
    "# 4 Prediction function\n",
    "# --------------------- \n",
    "def classify(text):\n",
    "    prompt = PROMPT_TEMPLATE.format(text=text.strip())\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=2,\n",
    "            temperature=0.0,\n",
    "            do_sample=False\n",
    "        )\n",
    "    decoded = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    # Extract last digit (0 or 1)\n",
    "    if \"1\" in decoded[-5:]:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# -----------------------------\n",
    "# 5 Run predictions\n",
    "# --------------------- \n",
    "y_true = df_test[LABEL_COL].tolist()\n",
    "y_pred = [classify(t) for t in df_test[TEXT_COL].tolist()]\n",
    "\n",
    "# -----------------------------\n",
    "# 6 Evaluation Metrics\n",
    "# ------------ \n",
    "acc  = accuracy_score(y_true, y_pred)\n",
    "prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"binary\", zero_division=0)\n",
    "\n",
    "try:\n",
    "    auc = roc_auc_score(y_true, y_pred)\n",
    "except ValueError:\n",
    "    auc = float(\"nan\")\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "print(\"Accuracy :\", acc)\n",
    "print(\"Precision:\", prec)\n",
    "print(\"Recall   :\", rec)\n",
    "print(\"F1       :\", f1)\n",
    "print(\"AUC      :\", auc)\n",
    "print(\"Confusion matrix:\\n\", cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86d4a5fd-3d34-4e05-bd97-176342e7bfa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train embeddings: 100%|| 6406/6406 [23:48<00:00,  4.48it/s]\n",
      "Val embeddings: 100%|| 800/800 [02:59<00:00,  4.45it/s]\n",
      "Test embeddings: 100%|| 801/801 [03:01<00:00,  4.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings saved to embeddings_split.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# save_embeddings \n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "# -----------------------------\n",
    "# Config\n",
    "# -----------------------------\n",
    "MODEL_PATH = \"./llama_lora_clean\"\n",
    "CSV_PATH   = \"001_2802_merged_12000.csv\"\n",
    "TEXT_COL   = \"text\"\n",
    "LABEL_COL  = \"label\"\n",
    "MAX_LEN    = 128\n",
    "EMB_PATH   = \"embeddings_split.pt\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# -----------------------------\n",
    "# Load model + tokenizer\n",
    "# -----------------------------\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "model     = AutoModelForCausalLM.from_pretrained(MODEL_PATH, output_hidden_states=True).to(device)\n",
    "model.eval()\n",
    "\n",
    "# -----------------------------\n",
    "# Dataset split (80/10/10)\n",
    "# -----------------------------\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "train_val_df, test_df = train_test_split(\n",
    "    df, test_size=0.1, stratify=df[LABEL_COL], random_state=42\n",
    ")\n",
    "train_df, val_df = train_test_split(\n",
    "    train_val_df, test_size=0.111, stratify=train_val_df[LABEL_COL], random_state=42\n",
    ")\n",
    "# ~80% train, ~10% val, ~10% test\n",
    "\n",
    "def get_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=MAX_LEN).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_hidden_states=True)\n",
    "        hidden_states = outputs.hidden_states[-1]\n",
    "        emb = hidden_states.mean(dim=1).squeeze(0)  # mean pooling\n",
    "    return emb.cpu()\n",
    "\n",
    "print(\"Extracting embeddings...\")\n",
    "X_train = torch.stack([get_embedding(t) for t in tqdm(train_df[TEXT_COL], desc=\"Train embeddings\")])\n",
    "y_train = torch.tensor(train_df[LABEL_COL].values, dtype=torch.long)\n",
    "\n",
    "X_val = torch.stack([get_embedding(t) for t in tqdm(val_df[TEXT_COL], desc=\"Val embeddings\")])\n",
    "y_val = torch.tensor(val_df[LABEL_COL].values, dtype=torch.long)\n",
    "\n",
    "X_test = torch.stack([get_embedding(t) for t in tqdm(test_df[TEXT_COL], desc=\"Test embeddings\")])\n",
    "y_test = torch.tensor(test_df[LABEL_COL].values, dtype=torch.long)\n",
    "\n",
    "torch.save({\n",
    "    \"X_train\": X_train,\n",
    "    \"y_train\": y_train,\n",
    "    \"X_val\": X_val,\n",
    "    \"y_val\": y_val,\n",
    "    \"X_test\": X_test,\n",
    "    \"y_test\": y_test\n",
    "}, EMB_PATH)\n",
    "\n",
    "print(f\"Embeddings saved to {EMB_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b74d994b-1697-4a05-8b96-19a222bc7374",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_19728\\3655227430.py:23: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(EMB_PATH)\n",
      "Epoch 1/30: 100%|| 801/801 [00:01<00:00, 412.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 | Loss: 0.6122 | Val Acc: 0.6825, Prec: 0.6215, Recall: 0.7875, F1: 0.6947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/30: 100%|| 801/801 [00:02<00:00, 393.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/30 | Loss: 0.5525 | Val Acc: 0.7137, Prec: 0.6816, Recall: 0.7057, F1: 0.6934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/30: 100%|| 801/801 [00:02<00:00, 397.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/30 | Loss: 0.5161 | Val Acc: 0.7200, Prec: 0.7049, Recall: 0.6703, F1: 0.6872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/30: 100%|| 801/801 [00:01<00:00, 406.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/30 | Loss: 0.4856 | Val Acc: 0.6550, Prec: 0.7630, Recall: 0.3597, F1: 0.4889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/30: 100%|| 801/801 [00:02<00:00, 395.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/30 | Loss: 0.4552 | Val Acc: 0.7262, Prec: 0.6682, Recall: 0.8011, F1: 0.7286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/30: 100%|| 801/801 [00:01<00:00, 405.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/30 | Loss: 0.4196 | Val Acc: 0.7512, Prec: 0.7386, Recall: 0.7084, F1: 0.7232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/30: 100%|| 801/801 [00:02<00:00, 385.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/30 | Loss: 0.3844 | Val Acc: 0.7550, Prec: 0.7090, Recall: 0.7902, F1: 0.7474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/30: 100%|| 801/801 [00:01<00:00, 422.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/30 | Loss: 0.3405 | Val Acc: 0.7775, Prec: 0.7288, Recall: 0.8202, F1: 0.7718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/30: 100%|| 801/801 [00:01<00:00, 404.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/30 | Loss: 0.3111 | Val Acc: 0.7863, Prec: 0.7159, Recall: 0.8856, F1: 0.7917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/30: 100%|| 801/801 [00:01<00:00, 420.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/30 | Loss: 0.2800 | Val Acc: 0.7963, Prec: 0.7802, Recall: 0.7738, F1: 0.7770\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/30: 100%|| 801/801 [00:01<00:00, 418.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/30 | Loss: 0.2596 | Val Acc: 0.8013, Prec: 0.7419, Recall: 0.8692, F1: 0.8005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/30: 100%|| 801/801 [00:01<00:00, 410.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/30 | Loss: 0.2322 | Val Acc: 0.8000, Prec: 0.7966, Recall: 0.7575, F1: 0.7765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/30: 100%|| 801/801 [00:01<00:00, 406.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/30 | Loss: 0.2047 | Val Acc: 0.7987, Prec: 0.7500, Recall: 0.8420, F1: 0.7933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/30: 100%|| 801/801 [00:02<00:00, 395.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/30 | Loss: 0.1913 | Val Acc: 0.8025, Prec: 0.7209, Recall: 0.9292, F1: 0.8119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/30: 100%|| 801/801 [00:01<00:00, 400.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/30 | Loss: 0.1733 | Val Acc: 0.8175, Prec: 0.7931, Recall: 0.8147, F1: 0.8038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/30: 100%|| 801/801 [00:01<00:00, 408.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/30 | Loss: 0.1640 | Val Acc: 0.7963, Prec: 0.7125, Recall: 0.9319, F1: 0.8076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/30: 100%|| 801/801 [00:01<00:00, 416.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/30 | Loss: 0.1466 | Val Acc: 0.8300, Prec: 0.7895, Recall: 0.8583, F1: 0.8225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/30: 100%|| 801/801 [00:01<00:00, 424.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/30 | Loss: 0.1291 | Val Acc: 0.8225, Prec: 0.7622, Recall: 0.8910, F1: 0.8216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/30: 100%|| 801/801 [00:01<00:00, 401.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/30 | Loss: 0.1314 | Val Acc: 0.8387, Prec: 0.8020, Recall: 0.8610, F1: 0.8305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/30: 100%|| 801/801 [00:01<00:00, 422.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/30 | Loss: 0.1401 | Val Acc: 0.8313, Prec: 0.7613, Recall: 0.9210, F1: 0.8335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/30: 100%|| 801/801 [00:01<00:00, 411.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/30 | Loss: 0.1067 | Val Acc: 0.8313, Prec: 0.8204, Recall: 0.8093, F1: 0.8148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/30: 100%|| 801/801 [00:01<00:00, 422.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/30 | Loss: 0.1089 | Val Acc: 0.8375, Prec: 0.8376, Recall: 0.8011, F1: 0.8189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/30: 100%|| 801/801 [00:01<00:00, 406.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/30 | Loss: 0.0894 | Val Acc: 0.8538, Prec: 0.8019, Recall: 0.9046, F1: 0.8502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/30: 100%|| 801/801 [00:02<00:00, 386.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/30 | Loss: 0.1037 | Val Acc: 0.8413, Prec: 0.8000, Recall: 0.8719, F1: 0.8344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/30: 100%|| 801/801 [00:01<00:00, 419.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/30 | Loss: 0.0893 | Val Acc: 0.8450, Prec: 0.7985, Recall: 0.8856, F1: 0.8398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/30: 100%|| 801/801 [00:01<00:00, 411.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/30 | Loss: 0.0898 | Val Acc: 0.8287, Prec: 0.7590, Recall: 0.9183, F1: 0.8311\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/30: 100%|| 801/801 [00:02<00:00, 382.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/30 | Loss: 0.0945 | Val Acc: 0.8313, Prec: 0.7578, Recall: 0.9292, F1: 0.8348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/30: 100%|| 801/801 [00:02<00:00, 381.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/30 | Loss: 0.0649 | Val Acc: 0.8087, Prec: 0.8474, Recall: 0.7112, F1: 0.7733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/30: 100%|| 801/801 [00:02<00:00, 373.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/30 | Loss: 0.0844 | Val Acc: 0.8650, Prec: 0.8151, Recall: 0.9128, F1: 0.8612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/30: 100%|| 801/801 [00:02<00:00, 395.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/30 | Loss: 0.0710 | Val Acc: 0.8625, Prec: 0.8320, Recall: 0.8774, F1: 0.8541\n",
      "Classifier saved to mlp_classifier.pt\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAArMAAAHWCAYAAABkNgFvAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjYsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvq6yFwwAAAAlwSFlzAAAPYQAAD2EBqD+naQAAfFBJREFUeJzt3QdcldUbB/AflylTEQVF3AMnKipuza1prsrU0qy0HGVZWVpqtqx/pZVplmXbnavMvffeA7c4WC5AQOb9f57zehEUFBR47+X+vp/P6d773nW4B/K5533Oc2yMRqMRREREREQWyKB3B4iIiIiIHhaDWSIiIiKyWAxmiYiIiMhiMZglIiIiIovFYJaIiIiILBaDWSIiIiKyWAxmiYiIiMhiMZglIiIiIovFYJaIiIiILBaDWSKiuzz//PMoW7bsQz33gw8+gI2NTa73iYiIMsdglogshgSJ2Wnr16+HtQbh6T8Hd3d3BAQE4KuvvkJCQoLe3SMiyhM2RqPRmDcvTUSUu/78888Mt3///XesWrUKf/zxR4bjbdu2hbe390O/T1JSElJTU+Ho6Jjj5yYnJ6vm5OQEPYLZ2bNn46efflK3b9y4gb///lsF97169VL3EREVNAxmichiDRs2DFOmTMGD/jcWFxcHZ2dnFHQSzM6fPx83b95MOyZBeVBQEHbv3o1Lly6hZMmS9zxPPr9bt26hUKFC+dJPaxkPIsofTDMgogKlZcuWqFGjBvbs2YPmzZuroGn06NHqvsWLF+Pxxx9XAZ3MulaoUAEfffQRUlJS7psze+7cOXXa/ssvv8SPP/6onifPr1+/Pnbt2vXAnFm5LYH3okWLVN/kudWrV8fy5cvv6b/MotarV0/N7Mr7/PDDD4+Uh2swGNRnYvo5hPxsnTt3xooVK9R7SRAr7yPOnDmDp556Cp6enuqza9iwIZYuXXrP654/fx5PPPEEXFxcULx4cbzxxhvq9e5O87jfeEjqw7hx41CxYkX1mfj5+WHkyJH3pETI7HvTpk1RuHBhuLq6okqVKmmvYTJ58mT1mcrrFylSRP1cM2fOfKjPjIgsi53eHSAiym1Xr15Fx44d8cwzz+DZZ59NSzn49ddfVTA0YsQIdbl27VqMHTsW0dHR+OKLLx74uhIcxcTE4OWXX1ZB2//+9z/06NFDBYD29vb3fe7mzZuxYMECDBkyBG5ubvj222/Rs2dPhISEoGjRouox+/btQ4cOHVCiRAmMHz9eBdkffvghihUr9kifx+nTp9Wl6X1EcHAwevfurX6WgQMHqgAxPDwcjRs3VjOnr732mnr8b7/9poJWmfHt3r27em5sbCxatWqF0NBQDB8+HD4+PuqzWbduXbbHQ2aM5XXlcxk0aBCqVq2KQ4cOYdKkSThx4oQK/MWRI0dU4F2rVi31WUjQe+rUKWzZsiXt9adPn676++STT6r+yCzzwYMHsWPHDvTp0+eRPjsisgCSZkBEZImGDh0q+QUZjrVo0UIdmzZt2j2Pj4uLu+fYyy+/bHR2djbeunUr7Vj//v2NZcqUSbt99uxZ9ZpFixY1Xrt2Le344sWL1fF//vkn7di4cePu6ZPcdnBwMJ46dSrt2IEDB9TxyZMnpx3r0qWL6sulS5fSjp08edJoZ2d3z2tmRvrt4uJijIyMVE3e79NPPzXa2NgYa9WqlfY4+dnk9ZYvX57h+a+//ro6vmnTprRjMTExxnLlyhnLli1rTElJUce++uor9bhFixalPS4+Pt7o7++vjq9bt+6B4/HHH38YDQZDhvcS8jh5/JYtW9TtSZMmqdvy82Sla9euxurVqz/w8yGigolpBkRU4Mjs3YABA+45nj4nVGZYr1y5gmbNmqmZyOPHjz/wdWURlZzCNpHnCpmZfZA2bdqotAETmWmUagOm58os7OrVq9GtW7cMea1yCl5mNbNLZk1lJleaPFdOxzdq1AgLFy7M8Lhy5cqhffv2GY79999/aNCggTqlbyIz2DJzKikKR48eVcckPcLX11fNrJpIWoTM8GZ3PObNm6dmY/39/dU4mJrM+ArTLK+kFphSRGQ2NzPymIsXL96T8kFE1oHBLBEVOBJoOTg43HNcTlnLqXIPDw8VSErAJ6e9RVRU1ANft3Tp0hlumwLb69ev5/i5puebnhsREYH4+HgVgN4ts2NZkaBSckylbdy4ERcuXFCn5MuXL39PMJtZHqykG9xNgk7T/aZLCczvzuPNqp+ZjcfJkyfVeJgCb1OrXLly2udh+gLRpEkTvPTSSyo9QVIV5s6dmyGwfeedd1TQLYF4pUqVMHTo0AxpCERUsDFnlogKnMxW5UuZqhYtWqggVnIvJRiTwG/v3r0qGMpq1i89W1vbTI9npyjMozw3J+R9ZBb4QfKrckFW7yWfd82aNTFx4sRMnyOLwUzPlaBcZmplIZrMCs+ZM0fN4K5cuVL9vBJsSw7wv//+q+6XcmRTp05V+dCSe0xEBRuDWSKyCrLCXhYiySIsWVVvcvbsWZgDqQggwbUsbrpbZsfyQpkyZVRQeDdTCobcb7qUlAMJxNPPzuakn/Jl4sCBA2jduvUDKzVIRQZ5nDQJfj/99FO89957KsA1Be5SVUFmcaUlJiaqhXmffPIJRo0apUvNXyLKP0wzICKrYJoZTT8TKkGPzOCZA9OMqqziv3z5coYAcdmyZfnSh06dOmHnzp3Ytm1bhhxcKUcm5byqVaumjkmurdSsXbJkSdrjpIKAVBXIrqefflq9RmbPkXQLeV9x7dq1e+6vXbu2ujSV8JIvKelJSoP0VcZaNsAgooKNM7NEZBWk5JTkqPbv31+VcZLZQNk5zJz2jZF6snLqXHJEBw8erBaFfffdd6pO6/79+/P8/d99913MmjVLLTiTz0hqzUppLpm9llP3MkMqpJyX9EtKe0kpLCkl9tdff6XNgGanJu5zzz2ncl9feeUVNcMqP7P8vDILLMdNNXAlJUTSDKQ+sMwISy6tfAEpVapU2kK1du3aqfJg8hqSV3vs2DHVP3mOlEEjooKNwSwRWQWpmSo5lW+++Sbef/99FdjK4i85dX33qn69BAYGqlnYt956C2PGjFF5oxLMSXCWnWoLj0oCwa1bt6ocYtmEQGZbperCP//8owJDE1ON3ldffRXffPONut2vXz/1hUFq52bntL4ExjILLXVlZVtiqbYgGx7IQjUJkE0LwaRiglRSmDFjhqp24OXlpXKfJRdWFvKZgmsJpiUFQXY/k0BXgnEZZyIq+LidLRGRmZNyXbLyXyoAmLOvv/5a7QQmZbKkggERUX5gziwRkRmRfNH0JICV+q+mLWnNtZ8yiytb4kppLAayRJSfmGZARGRG5DT7888/ry6lnuv333+vFjSNHDkS5kSqBUjtXFmMJTV6//zzT5UKIaf7iYjyE4NZIiIz0qFDB7UIKywsTO2cJbt3SSkqmfE0J5Jn/NNPP6ngVRZuSfWA2bNnq9JYRET5iTmzRERERGSxmDNLRERERBZL92B2ypQpqhi3lHIJCgpSBbuzIsWv029DGRAQoLYuJCIiIiLrpGvOrOyvPWLECEybNk0FslLWRfKwZDtF2drxblIzUBYZyI4x/v7+qqh29+7dVV3EOnXqZOs9ZT9w2V1HCmlnp7A3EREREeUvyYKNiYlByZIl0zZsud+DddOgQQPj0KFD026npKQYS5YsaZwwYUKmjy9RooTxu+++y3CsR48exr59+2b7PS9cuCA5wmxsbGxsbGxsbDDvJnHbg+g2Myt7ou/ZswejRo1KOyaRt+xNnn5f8PRkH+67d5YpVKgQNm/enOX7yHNM+3cL03o32Z4xq20OJZ1Btld87LHHYG9vn+OfjR4dx0B/HAPzwHHQH8dAfxwD6xuDmJgYlCtXLltbUutWzUBO9UthbUkRkNIzJlJLccOGDdixY8c9z+nTpw8OHDigtkCUvNk1a9aga9euqixM+oD17r3OZdvDu82cOVNtnUhERERE5iUuLk7FfVLH2t3dveDUmZU9wAcOHKjyZSXfVQLaAQMGqD27syIzv5KXaxIdHa32O2/Xrl2WH458+1i1ahXatm3Lb4A64Rjoj2NgHjgO+uMY6I9jYH1jEB0dne3H6hbMenl5wdbWFuHh4RmOy20fH59Mn1OsWDE1KyvbJl69elUlBb/77rtqp5ysSNFxaXeTgXjQYGTnMZS3OAb64xiYB46D/jgG+uMYWM8Y2OfgPXQrzSXbMwYGBqpUgfSVBuR2+rSDzEjerKQoJCcn4++//1apBkRERERkfXRNM5DT//3790e9evXQoEEDVZorNjZWpQ6Ifv36qaB1woQJ6rbk0V66dEntBS6Xkg8rAXBu71kuacSyGE3ycCUfl/Q5nWFnZ6dm4c1tDOSMgvSNpd2IiIisPJiVPbwjIyMxduxYtQ+5BKmyCYK3t7e6PyQkJENtMQlspNbsmTNn4Orqik6dOuGPP/5A4cKFc7XKggTKJUqUUO/PgEUf8oVC0k0uXLhglmMgiwfld0TOMBAREZF+dF8ANmzYMNUys379+gy3W7RogaNHj+ZZX2SWV0p2SQAt+bgeHh5qFo7yn4zFzZs31ZeWBxZLzucgW77wyJcw+V2pVKmSWfWPiIjI2ugezJoTCVIkiDLl40oNWwYq+pBxkPGQ/GhzGwP5vZDE9PPnz6f1kYiIiPRhXlGCmTC34InMD39HiIiIzAP/RSYiIiIii8VgloiIiIgsFoNZylLZsmVVuTQiIiIic8VgtgCQ0lX3a1KP92Hs2rULgwYNeqS+tWzZEq+//vojvQYRERFRVljNoAAIDQ1Nuz5nzhxVtzc4ODjtmJS3Sl9aSjYhkKL/DyLbBxMRERGZM87MPoAEf3GJybo0ee/skM0FTE1q48psrOn28ePH4ebmhmXLlqntgx0dHbF582acPn1abQMsG1RIsFu/fn2sXr36vmkG8ro//fQTunfvrjYNkBqrS5YseaTPV7Yjrl69uuqXvN9XX32V4f7vv/9evY+Uv5K+Pvnkk2n3zZ8/HzVr1lSlsooWLYo2bdqoHeSIiIis3pVTwC+dgJ/aAn/0AOb2B5a8Cqx4D1j/ObBtKrD3D+DoYuD0WuDiHiDyBBAdCiTGSgAES8GZ2QeIT0pBtbErdHnvox+2h7ND7gzRu+++iy+//BLly5dHkSJF1M5asoPaJ598ogLJ33//HV26dFEzuqVLl87ydcaPH4///e9/+OKLLzB58mT07dtX1Vv19PTMcZ/27NmDp59+WqVByG5wW7duxZAhQ1RgKlsZ79u3D8OHD1e7vDVu3BjXrl3Dpk2b0maje/furfoiwXVMTIy6L7tfAIiIiAqshBhgdm/gyomHfw0bW8DRDXB0V5e2jm4IiooHomoCXuVhThjMWokPP/wQbdu2TbstwWdAQEDa7Y8++ggLFy5UM61Z7cgmnn/+eRVEik8//RTffvstdu7ciQ4dOuS4TxMnTkTr1q0xZswYdbty5cpqhzcJlCWYvXjxIlxcXNC5c2c1u1ymTBnUqVMnLZiVjS169OihjguZpSUiIrJqRiOwaLAWyLqVBDpMAJLigYTo2y0GuHX70nQ77djtZkwFjCnArRtau30q3wdAktxnZhjMPkAhe1s1Q6rXe+eWevXqZbgtW8XKjOjSpUvTAsP4+HiEhITc93Vq1aqVdl0CTXd3d0RERDxUn44dO6ZSHdJr0qSJSm2QvF5ZPCaBqswmS7AszZTiIIG4BMISwLZv3x7t2rVTKQgy60xERGS1Nk8Cjv0D2DoAvf4ASmX89z9bwXBS3F1BbxSS427g0K4tqOFifutpGMw+gOSJ5tapfj1J4JneW2+9hVWrVqnUg4oVK6q8UwkGZXvW+5FtXO/+fGTr2bwgs7G7d+/Gxo0bsXLlSrWwTQJwqbJQuHBh1X9JTZD7JOXhvffew44dO1CuXLk86Q8REZFZO7UGWPuRdr3TFzkPZIWNDeDgojU3mYvVGJOSEHLGFjXsnWFuuADMSm3ZskWlDMhMp8xuymKxc+fO5Wsfqlatqvpxd78k3cDWVpuVlqoLsrBLcmMPHjyo+rh27dq0QFpmciWPV/JrHRwcVKoEERHlQNw1YNfP2mKhH1oAq8YBZzcByfef3CAzc/088PeLWopA3X5A4POwFpY/5UgPRSoELFiwQC36kqBQ8lbzaoY1MjIS+/fvz3CsRIkSePPNN1UVBcnXlQVg27Ztw3fffYepU6eqxyxfvhzh4eFo0aKFSh/477//VB+rVKmiZmDXrFmj0guKFy+ubsv7SIBMREQPIIHqqVXAgVnAiRVASrrANXQ/sOVrwMEVKNcCqNgaqNgGKKKtTyAzlBQPzHkWiL8OlKwLdPwC1oTBrJWSxVcvvPCCqhLg5eWFd955B9HR0XnyXjNnzlQtPQlg33//fcydO1elD8htCXBloZrMGEvQKmXGpk2bpmZeb926pQLwWbNmqVJekm8r6QeSXyv9ltxaKevVsWPHPPkZiIgsnuRCXtqrBbCH/wbir925z7sGUKsX4OqtlWk6tRqIuwIEL9WaKFpJC2qllW0C2BfS7Uehu8b13zeAsIOAs5eWJ2vvBGvCYLaAkUBQmoksosqsXJXUdDWdrjcZOnRohtt3px1k9jo3bmirHLOyfv36+97fs2dP1TLTqFEj1UeD4d5sGJmBlZlbIiJ6gBshwME5wIE5wNWTd45L4FrzKSDgGcAnXTWYgF6AnKmT4EiCWsnDvLBDe660Hd8Ddk5AmSZ3gluvSlquZV6RBUlmuIreLOz6SfuCIqW0nvoF8CgFa8NgloiIqKCRVehSDP/AbOD85jvH7QoBVTtrAWy5loBtFmGATCKUrK215m8B8TeAsxtvB7ergehLwOk1WlsxCvAoraUjVGoLlGuu1Se9n6Rb2sxvbCQQe1W7TLstl1fSHbsC+6Q4tLdzh8F+G1CnD1AiIG+DZ0sRsh1Y/q52ve2H2mdvhRjMEhERFQQpycCZddos3fGlQPKtO/eVbaYFsFWfAJzcc/7ahQoD1Z7Qmpyliwy+E9ie3wJEhQB7ftGawQ4o3UibuZVc3NsBaVqAKpeJMTnuglNyNLDrB60Vq6rNINd8GvDwhVWKCQPm9gNSk4HqPYBGGc+uWhMGs0REVPBOq0ccBzzLAUXKZT37WBBIYBl2SEsjODgXiE1X99urshbASsBX2C/33lNmRIv7a63xMG3r03Nb7gS3104D5zZp7X4M9oCL1+1WTMv3lEuXorcvTce8kGTvht0LpyLI6TQMwcuAyGPA6g+A1eO12ciA3kDVLoCjK6xmAZ9sT3szHCheDej6nVXPVBfgv3AiIoK1n1a3ddSCOgm8ikkAVlVrhctqp9ItKWiVGc1rZzI2CWSvBN95nHNRoMaT2qylrGrPjwBH6pFWbqc1If2SPNvL+7SKCCowvTto9QKcPLLfv6QkRHgEIKXTKBiSY7WxlgBeZoXPbtDa0hFaQKtSKFoAhtzbeMjsrBgNXNgOOHoAvf7UxsCKMZglIqICdlrdRgtgoy5oC4fCD2ktPckdLVZZO12tZhmracGuh59+Qa4ErDLTlj5YvXr69vWzWZ+al52eqnQEaj2jLcayc4CuPMsDDcrn3etLykNgf61dPwccnKf9DsiMsJqhngO4lbi9uK034F0NBcr+mcCu6dr1Hj8CRSvA2jGYJSKignlaXVbk3zgPRB4HIo7duZQ965PjgdADWkvP3gUoVuXODK4Eu54VYZcie9vHACm58M9mQvS9M6wSrMqlBN9ZstFWqhcpqwWMplauGVDISrfyls+ixdvaIrWLu4GDs4FD84GYUGDrt1rzqXX79+IpwLV47v8+ypeo/CpTdnm/VoZLtBwFVOmQP+9r5hjMEhHRw+02dPxfrbyTzGhKaSY7x7x7v+hQ4NBcrbxUxJFMTqs/A5Ssk/G0tcywSt6sNJm5NElN0Wb0VIB7TMuvletSdiopFri8V2u3ySbej8uVg8h7NgagcOmMwaqpFS5jdfVDs03G3a++1tp/CpxcqaWcyIYQUmJM2soxWsUFqafrLyNqo31BkS8X0iRVRd3O7Fi6+9KORWmXUjLMuybQ8l3tdfMqtUN2apvznBY8V+4ANB+ZN+9jgRjMEhFRzhz7B1g0VPvH3ERqXMrpTlNeqrqsph2zlXDwIcjCIkkfkFPIZ9bfqTNqOq0up5DltHpOX19yKaVf0qRMVfq0BZkdTQtwj6rZXOPVU7CRFeO5RVb73z27amqS5qB3moClky9VkjsrTQJA2SBCZvIv7tKCXGm5TdJY5vTVZoEfG60Fm7kZ1MoXsPkvaFUjZFFj9x8sK+c7jzGYJSKi7K+glhXk26dot2U2ysFZm9WUmSs5fS/t2JKMK9aLVkx32v52sJtVlQH5R1tWwcsMrLxO4s079/k11GZgq3fLm9Pq0h+VR1sZqNb1zo+dcAvL//sXHTp0gL39QwbmdwezBXlxkjlx9gQaDNTalVNaGoIEtlLxwsTBTauLKyXL5FK129dlkVr622mPMzW3OxsX7JimzQDPun2WoOVore5ubgS1az/S8sPtnYFn/tLyhikNg1nKsFtY7dq11RaxREQZRF0E5g0ALu7UbjcaBrT5QJsVlbzB6Mt3ZjTTLo9rwajclpYuO+CeKgNyXU7tSx6sFOQ3kRlMmYGt9bQ2c6kHgy1SJSiXGT+7XAhmSR9eFYFW72tBpiy0ky9iUm0hN75YtB4DNBwCbJsM7PhRq+Qw8ynAtx7w2CigQuuHD2qlcsPmSdr1JyYD3tUfvb8FDIPZAqBLly5ISkrKdHvXTZs2oXnz5jhw4ABq1aqVK+8XHx8PX19ftc3spUuX4OiYh3lyRKS/k6uABYOA+GtaKaBuUzOenpd/pKVwvTQ57W8iQa5UFMgQ4EpgG5x1lQEhs2FSBF6CWL8GVl0/k/KAnJ53L5H7ryv1ceULnnzR2/INsHM6cGk38GdPwC9IW7BVvmXOfp/lb2XREO26vG7NJ3O/3wUAg9kC4MUXX0TPnj1x8eJFlCqVcU/mX375BfXq1cu1QFb8/fffqF69OoxGIxYtWoRevXpBL9KHlJQU2NnxV5ko10kO6foJwKYvtduyhehTv2kLqrJD/tGWxUzSTDVIRWZVBuQfbXdfrT5qpfZc6ESWS2rotvsIaPwqsPlrYPfPwIUdwB/dgNKNtZxaqUDxILLQbHZf7eyG7ODWZnx+9N4iMXv4QWRmQRYh6NHkvbOhc+fOKFasGH799dcMx2/evIl58+apYPfq1avo3bu3mlF1dnZGzZo1MWvWrIf6SH7++Wc8++yzqsn1ux05ckT1yd3dHW5ubmjWrBlOnz6ddv+MGTNUMCwzuiVKlMCwYcPU8XPnzsHGxgb79+9Pe+yNGzfUsfXr16vbcim3ly1bhsDAQPUamzdvVq/ftWtXeHt7w9XVFfXr18fq1asz9CshIQHvvPMO/Pz81PMqVqyo+i8BsVz/8svb/2DfJv2Q9zp16tRDfU5ED02K4+/6Gfi1M+w+90PQ6a9gYzq9n19iwrV/fE2BbL0XgRdWZj+QvR9TlQFZxNVshFYr8+UNQO+ZWq4qA1kqCKQMWIdPgeEHgKBXtNSakK3Ab53V3zbOb836ufKFb+ErWoUN+ZL35C8Feye7R8RP5kHkVNinJfV579GXs7Wrh8xK9uvXTwWz7733ngrAhASyMmspQawEthL8STAnQebSpUvx3HPPoUKFCmjQoEG2uyRB47Zt27BgwQIVBL7xxhs4f/48ypQpo+6XtANJa5D827Vr16r32rJlC5KTtZXA33//PUaMGIHPPvsMHTt2RFRUlLo/p959910VfJYvXx5FihTBhQsX0KlTJ3zyyScqUP39999V+kVwcDBKly6tniOfkfT922+/RUBAAM6ePYsrV66oz+uFF15Qs9hvvfVW2nvIbflZJNAlynOy6loWPB1ZCJzdBBhT1GH5a/aJPgD81kmbnZHgr/xjeXvqXd5fVk5LDVepu9rlG6DWU3n3fkQFmZsP0PFzoMlwYNNEYO9v2iLHXzpqaQeSw1s6KONzNk8EgpdqlTue/gNwLaZX7y0Cg9kCQoKxL774Ahs2bFCBpCkYk/QDDw8P1dIHaq+++ipWrFiBuXPn5iiYlVlVCUIlgBTt27dX7/PBBx+o21OmTFHvNXv27LRVv5UrV057/scff4w333wTw4cPTzsms6g59eGHH6Jt27Zptz09PVWAavLRRx9h4cKFWLJkiZr5PXHihPpZV61ahTZttJw+CYRNnn/+eYwdOxY7d+5Un4fkIM+cOfOe2VqiXBV/XSs9dXjB7dJTWgCrlKgN1OiB5BKBuLj0fyhzfStsTPvdyzalzd4EqnTK3fI8Mhsk/4iu+0QrgyUbBjz9u7a6n4gejXtJ4PEvbwe1XwH7/tT+7qXJAjFJPyhVDzi1Glj7sfacTl8CpQL17rnZYzD7IFIGQ2ZI9XrvbPL390fjxo1VsCnBrJwal8VfEvQJmaH99NNPVUAns6eJiYnqtLukHGSXvMZvv/2Gb775Ju2YpBpIkCyBoCwIk1PzklaQWfmaiIgIXL58Ga1bt8ajkjzg9GTmWQJqmXEODQ1VM8GyUC0kRCu/Iv2ytbVFixYtMn29kiVL4vHHH1efnwSz//zzj/p8nnqKs1GUy25FAcf/02ZgT68FUpPu3OdTE6jeXWu3V+4bk5JwoPSL8O3zLex3TgP2/Kqt+pealhJsykytLJZ61FOQMjMsi7xOrdJuB/QBHv9KW/FNRLlHdqfr8jXQ9A0tjUe2pz29RmsV22r1cGEEAp/XtuylB2Iw+yByKi8bp/rNgeTGyoyrzI7KbKmkEJiCN5m1lSBUym5JvqyLiwtef/11FdRml8zkSiB894IvCXLXrFmjZkoLFcp6S7/73SckGBaSvmAiM6SZkf6nJwG1zLrKTKqkBch7Pfnkk2k/34PeW7z00ksq9WLSpEnq85OfMyfBPlGWZJeg4GVaACuzLinp/u6KV78TwErpoKxI3lzHz7RtO7dP1VZKS2WABQO1WZymrwO1+z7cLlwXdmplt6IvAnZO2mxQ3ece7mclouwpUkYrtdV0BLDxS21zENOXSd9AoOP/9O6hxdB9AZgEXmXLloWTkxOCgoLUad77kWCsSpUqKjiRhTySs3nr1q186685e/rpp1VAKKfHJWdUUg9M+bOSlyoLpGQmVU7Hyyl2OfWeE7JY6plnnlGznOmbHDMtBJOqCTIjnFkQKovBZKwl8M2MLGITMrNqkn4x2P3IzyepAt27d1fBuo+Pj1pQZiLHUlNTVRpGViTnVoJkyeuVMmfy+RE9tISb2h7xshr5fxW0oDP4Py2Q9aqilekZuhMYslXbW/5+gezdK6VbjwXeOKxdOntplQFkv/ZvAoCt32kLSLNDvjhum6rl7kkg61kBeGkNA1mi/CSLIbtNAYbt0r6Qlmuu5cnm5fbQBYyuM7Nz5sxRi4GmTZumAlkJVCUHUxbtFC9e/J7HS5AmC3/kVLCcUpdgTAIYCdgmTpwIayer+GU2cdSoUYiOjlafjUmlSpUwf/58bN26VeW7yucVHh6OatWqZeu1IyMj1al3yUGtUaNGhvtkYZUEkdeuXVP5qZMnT1YBrvRD8me3b9+uTt3LlxBJBXjllVfU+ErubUxMjApEZUZZvqA0bNhQLQ6TBWUSjJrSJB5Efj5ZlCaLvuT3YcyYMSp4NZEgun///ipANS0Ak4VrkvogXwKEpCHIZyb9ltdr1KhRNj95surSVXFXgdhIrZmuh2wDTqwEkuPvPFZ2wZJ0AJmBlR2wHnUBl9RilbzZoMHA3t+Brd9qmw2sfE/Lx2s4WNv1KKudsiTdYfFQbWtaUa2bNkskuxsRUf6T7ZWlhjNZVjArAdXAgQMxYMAAdVuCWsl5lGBVgta7SSDWpEkT9OnTJy1AkZX6O3bsyPe+m3OqgcySyiyj5IGavP/++zhz5oz6siCnzgcNGoRu3bqpagLZITO9MmuZWb6rHJNA9M8//8Rrr72mqhi8/fbbKsVBAkTZVUzGTUhAKTPpcipfUgO8vLxUOoCJjL38DLIoTNIFJD1CtpDMzu+SBKryJUdeU6o2SECfnsy4jh49GkOGDFGlyqTKgdy++/OT3GLT7yTlQEoSEBOq7QQlu0VJYBV1SbtU7TIg+9tLeRrZe15d3m6ZHnNId9/dx+TSKeOx9I/N8pjT/XNLZStVWZSlgtMrdy7j0l03HZdj8tj7kS1bJXit0QPwrpE3FQgkp7XhK0C9F7StOmWnoGtntEVcW74F6r8INBqqlQkyCT0AzO0HXD+nbTfb/lMt8OXmBERkgWyM6RMU85HkMkpQJbOFElSZSLAjtUUXL16c6cysBCIrV65UM30SnMmiHclzvDsoMZFFPNJMJMCR9AQpySRlo9KTIEtKPMmsoJwml9PiptP0lL/k11JmbfN7DCRFQnJ/ZdZWatZmRX5XZOZYfpckRaYgkr8ByUOWz8Pe1gDcjIBNjBaY2twOTm0kQDXdjo2AjayAN3NGG8O9wbPBTstrjb+W459BvV4hT3X63+hcVJ32NxYph1T/zoBPwCMHiBnGIZOFlfdITYHNscWw3fo1bCKOan20c0Jq7WeR2nAoDKfXwLDyPdikJMDo4YeUHj/DKNURKPfGgHIdx8D6xiA6OlpNTsmk293xmtkEs7KqXQr4y2xr+tO5I0eOVHmNWc22yilimdGTbsuKdTllLTNuWZHT2uPHj880ML57cY/Ua5VcSwlQHBwcHunnI8siX3jkC458WZIUiOnTpz/wy5h88QkLC0uroVsgGI0olHgFnrEn4Rl7Ch7x51Eo8Rqckm7AgHRlo7KQamOLeHtPxDt4It6+KG7ZF0G8Q9Hbtz2RamMHgzEJhtRk2MqlMRmGVLlMgm1qsnZf2v23b6feedzdx2xN92XxmqZjNrIyOAcSbV2QYOeuWqK9XLpp129fpr+eaOcKSEBrboxGeEfvR+WwJfCM0zYtMcIm7bMIc6+NvWUGIUn6T0RkZuLi4tSZ+OwEsxZVzUB2f5JTwFOnTlU5tlJ+SuqVSk1RyZHMjOQ/Sl7u3TOz7dq1y3JmVk6nc2bWumZmZcMJSXmRlAhJl3jQH478rkhqhWyqYNEzs8m3YBN2EDYXd2nt0i7Y3AzP9KFGG1vArQSMsqre3XTpC6ObXJaEUWoouhSDg40B8lXQI99/mEz6LD+iXJH0huQEraVISwSSE9V1G3UsEUZHd21xVSFP2NjaQ0bVyeJnQx4HjKORfH4zDFsmwXBuoxrH1MfeR9GGQ9HWHINwM8RZQf1xDKxzZja7dAtmZepY8illEVJ6cltmRzMjAaukFEgJJdMK9djYWJX/KTtfmUo7pSe7QUm7mwzE3YMhJaYkcDIFT3KZ2WtS3jMt3sqvMZB825xUL5A+Sd8y+z0ya9GhgGyLeuF2C92fsUyUkFPuJQKQ4lsP+8INqP1YV9h5loGNqzdgsFU7Ulkee8DxweXZzNkj/a5VaqW1sEOwMdjDtrg/bHO7g1bA4v7eCyCOgfWMgX0O3kO3YFZO48v2qlKmyZQzKwGM3JYV8VlNOd8d2EhALHTKliAy7wVZ4YfvBK7SorRNJDJwKQaUagD4SQsCStYG7AshNSkJl/77DwG+9eT/Knr8BJTbZFMGIqICRtc0Azn9Lwu+ZDcnWdAlpblkptW0klxKPkle7YQJE9RtKbskq9br1KmTlmYgs7Vy3BTU5gYGxmRxvyPSH1mMJTOtF3drgavsEpUUl/FxclpZivSbAle/+tqKe6bTEBGRhdI1mJWaqFK/VLZClYU0kq8oxepNq8hlK9L0M7FSXkpO7cql7EQlRfYlkP3kk09ydUpbZoB5GoPuR35HhC6/JxK43gjRyitJ8KouD2jlojKrRZo269pA21XG0S3/+0xERJRHdF8AJikFWaUVyIKvu6sNjBs3TrW8ILO7hQsXVgG2LDySQCU3Z3wp+yTlRCoGyEIrc8pblhlZCWRlswX5Xcnz3w8JXKVm6N2Ba2b1TWWBVjF/oGQdoHSQFsR6VZYE37ztIxERkTUHs+ZGFp/JQjDZUlVW07OagX5BY3x8vKoYYI5jIIFsVgsVH5osert66q7A9SCQkMnGFlLoXnaRkvzWEgFAidqAd3WV60pERGRNGMzeRQInSXPYu3cvWrVqpWaDSZ8SIBs3blSlr8wt5SNXZ+wjjgF7/9DyW8MOAYk3732MFPaXQDUtcA0Ailfjvt1EREQMZu8/MyglvcwtkLIWEizKZgRSw7VAjkFkMLDhc+DwgtvVUG+zK6StOE8fuErqgG0B/AyIiIhyAYNZovx05ZQWxB6adyeIrdoFkK1PJXBVOa7M0yYiIsouBrNE+eHqaWDjF8DBOYBR2xBCBbAt32XtTyIiokfAYJYoL107C2z8EjgwCzCmaMcqd9SCWEklICIiokfCYJYoL0gdWAli9/8FpCZrxyq104JYqfVKREREuYLBLFFuiroIbPpKq1CQmqQdq9AaeGw0UKqe3r0jIiIqcBjMEuUG2Up200Rg729ASqJ2rHxLoOVobQMDIiIiyhMMZokeRUwYsPlrYPcMICVBO1a2mTYTW6ax3r0jIiIq8BjMEj2MmxHAlm+AXT8Bybe0Y6UbA4+NAso117t3REREVoPBLNHdjEbg1g0g9srtFqm1uKva5c1w4MRKIDlee7xfENBylJZWYIZb7xIRERVkDGbJOiTGATfDsg5QVbt9Pe7KnQoE9+NbT5uJlQVeDGKJiIh0wWCWCjYJUDd8puW0ZidATc/RHXDxAlyKAc5yaWrFgOLVtHQCBrFERES6YjBLBVNyArDzR2DDF0BClHbM3iVjQCqXzumu3x242jnq/VMQERHRAzCYpYKX73psCbBqLHD9nHZMtott9wlQvoXevSMiIqJcxmCWCo5Le4AV7wEh27Tbrj5A6zFAQG/AYKt374iIiCgPMJilgrHr1poPgYNztNt2hYAmrwGNXwMcXfXuHREREeUhBrNkuRJuarVet06+UyZLZmFbjQE8fPXuHREREeUDBrNkeVJTgP0zgbUfaTVfRZkmQLuPAd+6eveOiIiI8hGDWbIsZ9YDK94Hwg9pt4uUA9p9BPh3ZpksIiIiK8RglizDlZPAyjHAiWXabScPoMU7QP2BgJ2D3r0jIiIinTCYJfMWdw3Y8hWw+2dt0wODHVD/JS2QdfbUu3dERESkMwazZJ6SE1AhYhnsvn8VuHV704MqnYC2HwJelfTuHREREZkJBrNkflJTYTunN2pc2qjd9q4JtOemB0RERHQvBrNkfvb9AcO5jUg2OAAdv4Bd4HPc9ICIiIgyxWDWGqUkATdCgGtn1Ol8dfreYIBZuBmpbUUL4FiJJ+Ffuy8DWSIiIsoSg9mCKukWcOO8FrDe3W5cAIwpdx7bcjTQ8h2YhZXvAbduwOhdE2eLtYW/3v0hIiIis8Zg1pIlxgHXz2YSsJ7VtniFMevnypavHqWAqyeBjf8DKrcDStaBrk6vu70lrQ1SOn0F4/4wfftDREREZo/BrCVJiAE2fQVc2KkFrTGh93+8gxvgWQ7wLH9vc/PRHjO3H3BsCbDwFWDQBsDeCbrNJC8doV1vMAjGknWB/f/p0xciIiKyGAxmLYUEsAsGAtfPZTwumwd4Vsg8YHXxevCuWJ0nASHbgMjjwLpPtN209CBBugTobiWAVu/r0wciIiKyOAxmLWGx1sYvtGZMBTz8gJbvAsWqarOuj7pxgAS8Xb4FZvcGtk7WFoOVaYR8FRkMbJ6kXe/4OeDkDiQl5W8fiIiIyCKZyRJ2ytTV08CMDsCGz7VAtlYvYPAWoM6zQKnA3NsBy78TIFUDJMd20StAwk3kG6MR+PcNIDUJqNwBqPpE/r03ERERWTyzCGanTJmCsmXLwsnJCUFBQdi5c2eWj23ZsiVsbGzuaY8//jgKDAnw9vwGTGsGXNqtpRL0/Bno8aN2PS90mKDN+koaw6oxyDf7/wLObwHsnYFOXzw4LYKIiIjInILZOXPmYMSIERg3bhz27t2LgIAAtG/fHhEREZk+fsGCBQgNDU1rhw8fhq2tLZ566ikUCLFXgNl9gX9eA5JigbLNgMFbgZpP5u37SpDcdYp2ffcM4NRq5MvPuvJ2fmzLUUDh0nn/nkRERFSg6J4zO3HiRAwcOBADBgxQt6dNm4alS5dixowZePfdd+95vKdnxlPrs2fPhrOzc5bBbEJCgmom0dHR6jIpKUm1zJiOZ3V/XrE5tRq2/74Gm9gIGA32SH3sPaQGDQFsDPmTQ+rXGIZ6A2G7ezqMi4YiedBmoFDhPHs72xXvwRB/Hcbi1ZEc+FKGn1GvMaA7OAbmgeOgP46B/jgG1jcGSTl4HxujUc5p6yMxMVEFovPnz0e3bt3Sjvfv3x83btzA4sWLH/gaNWvWRKNGjfDjjz9mev8HH3yA8ePH33N85syZ6r3NgW1qAqpdmoPyV7TZ0GgnX+wpMxjRzqV16UvL42PgmhCGC0UaY2/ZV/LkfbxijqLJqc9ghA02VR6D6y4V8+R9iIiIyPLExcWhT58+iIqKgru7u/nOzF65cgUpKSnw9vbOcFxuHz9+/IHPl9xaSTP4+eefs3zMqFGjVBpD+plZPz8/tGvXLssPR74NrFq1Cm3btoW9vT3yVOgB2C1+BTayeYEUL6j/Mgo99j6a2heCXmzq+MH4Wyf4Xd+KEq0GwujfJXffIDkBdtO1LxipgQPQqMNr+o4BZYpjYB44DvrjGOiPY2B9YxB9+0y6RaQZPAoJYmVmtkGDBlk+xtHRUbW7yUA8aDCy85iHlpoCbPlGq+2amgy4+gDdpsK2YmvYQmdlGwFN31C1X+3+exMo1xRwLZ57r79FasqeBly9Ydv2A9je5zPO0zGgbOEYmAeOg/44BvrjGFjPGNjn4D10XQDm5eWlFm+Fh4dnOC63fXxu71CVhdjYWJUv++KLL8LiXD8P/NoZWDNeC2SrdgGGbAMqtobZaPEu4F0TiL8G/DNcq7CQG66c1DZIEB0+y7vqDERERGQVdA1mHRwcEBgYiDVr1qQdS01NVbclD/Z+5s2bpxZ2Pfvss7AYEhAemANMawqEbAUcXIGuU4Gn/8i9mrG5xc4B6D4NMNgDwf8B+2fmXk3ZlESgYhugevfc6CkRERFZMd1Lc0k+6/Tp0/Hbb7/h2LFjGDx4sJp1NVU36Nevn8p7zSzFQBaNFS1aFBYh/jow/wVg4SAgIRrwCwJe2QzU6Wu+tVV9agCPjdauL38XuHHh0V7vwGzg3CbArhDw+Ffm+3MTERGRxdA9Z7ZXr16IjIzE2LFjERYWhtq1a2P58uVpi8JCQkJgMGSMuYODg7F582asXLkSFuHMBmDRYCD6EmBjq9VUlZxUW90//gdrMhwIXgZc3AksHgI8txi4azyyJe4asPI97XrLd4AiZXO9q0RERGR9zCKaGjZsmGqZWb9+/T3HqlSpAh0riuXMqnHAlq+1654VgB7Tta1oLYXBVks3kNSIsxuBXdOBoJdz/jqyq1jcVaB4NaBR5mNNREREZHFpBgWe3e1KCoEDgFc2WVYga1K0AtD2wzvB+ZVTOXv+uc3Avj+1652/Bmy5EpWIiIhyB4PZvNb8baDfEqDL14CDCyxWvReB8i2B5Hhg4ctASnL2npecoC36MgX0pYPytJtERERkXRjM5jWZhSzfAhZP8mS7TgEcPYBLu++kTjzIlm+BKycAl2JAm3F53UsiIiKyMgxmKfs8SgEdP9eur/8MCDt0/8dfPQ1s/OJOTdlCRfK+j0RERGRVGMxSzgQ8A/h3BlKTgAUva2kE960pmwCUfwyo0TO/e0pERERWgMEs5YzUhpVFXM5eQMQRYP2EzB93aB5wdgNg5wR0nsiaskRERJQnGMxSzrkW0xa0iS3fACE77q0pu3zUnQVwnuXzv49ERERkFRjM0sOp2gWo9QxgTAUWvQIkxt65b/U4IO4KUMwfaPyanr0kIiKiAo7BLD08WQzm7gtcO6PVnxXntwF7f9eud54E2Dno2kUiIiIq2BjM0sMrVBjo+p12XXYGO7ES+Pd17XbdfkCZxrp2j4iIiAo+BrP0aCq0Auq/pF2f3RuIPK4tDmszXu+eERERkRVgMEuPTra6lUVeqbd3BWv/KeDsqXeviIiIyAowmKVHJ9v0dv8BsHcGKncEaj2td4+IiIjIStjp3QEqIPwaAG+f0urKsqYsERER5RMGs5S7M7RERERE+YhpBkRERERksRjMEhEREZHFYjBLRERERBaLwSwRERERWSwGs0RERERksRjMEhEREZHFYjBLRERERBaLwSwRERERWSwGs0RERERksRjMEhEREZHFYjBLRERERBaLwSwRERERWSwGs0RERERksRjMEhEREZHFYjCbx5JSUvH7tnPqkoiIiIhyl10uvx7d5eU/9mDt8QhcvnEL73b017s7RERERAUKZ2bz2NP1SqnLaRtOY11whN7dISIiIipQdA9mp0yZgrJly8LJyQlBQUHYuXPnfR9/48YNDB06FCVKlICjoyMqV66M//77D+aqQ40S6N+ojLr+5twDCIu6pXeXiIiIiAoMXYPZOXPmYMSIERg3bhz27t2LgIAAtG/fHhERmc9gJiYmom3btjh37hzmz5+P4OBgTJ8+Hb6+vjBnozpVRfWS7rgWm4jXZu9DMvNniYiIiCw/mJ04cSIGDhyIAQMGoFq1apg2bRqcnZ0xY8aMTB8vx69du4ZFixahSZMmaka3RYsWKgg2Z072tpjSpy5cHe2w8+w1fLvmpN5dIiIiIioQdFsAJrOse/bswahRo9KOGQwGtGnTBtu2bcv0OUuWLEGjRo1UmsHixYtRrFgx9OnTB++88w5sbW0zfU5CQoJqJtHR0eoyKSlJtcyYjmd1/8Pw9XDAR09UxRvzDmHyulOoW9oDTSoUzbXXL2jyYgwoZzgG5oHjoD+Ogf44BtY3Bkk5eB/dgtkrV64gJSUF3t7eGY7L7ePHj2f6nDNnzmDt2rXo27evypM9deoUhgwZon5gSVXIzIQJEzB+/Ph7jq9cuVLNAt/PqlWrkNvT4I2KG7AtwoBX/9qNkbVS4O6Qq29R4OT2GFDOcQzMA8dBfxwD/XEMrGcM4uLiCmZprtTUVBQvXhw//vijmokNDAzEpUuX8MUXX2QZzMrMr+Tlpp+Z9fPzQ7t27eDu7p7pcyQ4lsGS/Fx7e/tc/RlaJaWg57QdOBFxE8uuF8eM/oGwNdjk6nsUBHk5BpQ9HAPzwHHQH8dAfxwD6xuD6Ntn0s06mPXy8lIBaXh4eIbjctvHxyfT50gFA/kA06cUVK1aFWFhYSptwcHh3mlOqXgg7W7yOg8ajOw8Jqfk9aY+WxddJm/B1jPXMH3zebzaulKuvkdBkhdjQDnDMTAPHAf9cQz0xzGwnjGwz8F76LYATAJPmVlds2ZNhplXuS15sZmRRV+SWiCPMzlx4oQKcjMLZM1VxeJu+LhbDXV90uoT2H7mqt5dIiIiIrJIulYzkNP/Ulrrt99+w7FjxzB48GDExsaq6gaiX79+GRaIyf1SzWD48OEqiF26dCk+/fRTtSDM0vQMLIWedUsh1QgMn70PV2/eWaRGRERERBaQM9urVy9ERkZi7NixKlWgdu3aWL58edqisJCQEFXhwERyXVesWIE33ngDtWrVUvVlJbCVagaW6KNu1bH/wnWcjozFiLkH8Mvz9WFg/iwRERGR5SwAGzZsmGqZWb9+/T3HJAVh+/btKAicHewwpW9ddP1uCzaciMSPm87glRYV9O4WERERkcXQfTtba+fv444Pnqiurn+xIhh7zl/Tu0tEREREFoPBrBl4pr4fnggoiZRUI16duQ834hL17hIRERGRRWAwawZsbGzwSfcaKFvUGZejbuGteQdhNBr17hYRERGR2WMwaybcnOzxXZ+6cLA1YPWxcMzYck7vLhERERGZPQazZqSGrwfe71xVXf9s2TEcuHBD7y4RERERmTUGs2bmuYZl0LGGD5JSjBg2ay+i4pP07hIRERGR2WIwa4b5s5/1rIVSRQrhwrV4vPs382eJiIiIssJg1gx5FNLyZ+1tbbDscBj+3H5e7y4RERERmSUGs2aqtl9hvNPBX13/6N9jOHwpSu8uEREREZkdBrNm7MWm5dCmanEkpqRi2My9uJmQrHeXiIiIiMwKg1kzz5/98qkAlPRwwrmrcRi94BDzZ4mIiIjSYTBr5go7O2BynzqwNdhgyYHLmLPrgt5dIiIiIjIbDGYtQGAZT7zVroq6Pm7JERwPi9a7S0RERERmgcGshXi5eXk0r1wMCcmpeOm33TgRHqN3l4iIiIh0x2DWQhgMNpj4dAD8PAvh4vV49Ji6FauPhuvdLSIiIiJdMZi1IF6ujlg8tCmCynmqygYD/9iNqetPcVEYERERWS0GsxbG08UBf74UhGcblobEsP9bHozhs/cjPjFF764RERER5TsGsxbI3taAj7vVxEfdasDudpWDp3/YhtCoeL27RkRERJSvGMxasOcalsEfLwahiLM9Dl2KQpfJW7Dn/HW9u0VERESUbxjMWrhGFYpiybCm8Pdxw5WbCej943bM281atERERGQdGMwWAH6ezvh7cGO0r+6ttr59e/5BfPTvUSSnpOrdNSIiIqI8xWC2gHBxtMP3fQPxWutK6vbPm89iwK+7EBWXpHfXiIiIiPIMg9kCVot2RNvKmNq3LgrZ22LTySvoPnULTkfe1LtrRERERHmCwWwB1KlmCcwf3Ai+hQvhzJVYdPtuC9YFR+jdLSIiIqJcx2C2gKpe0gOLhzVB/bJFEJOQjBd+3YUfN57mBgtERERUoDCYLeA7hv31UkM8U99PbbDw6X/H8ebcA7iVxA0WiIiIqGBgMFvAOdgZMKFHTYx/ojpsDTZYsO8Sev24HeHRt/TuGhEREdEjYzBrBWxsbNC/cVn88UIDFHa2x4ELN9Bl8mbsv3BD764RERERPRIGs1akcUUvLB7aBJWKuyIiJkFtgbtw30W9u0VERET00BjMWpkyRV2wYEhjtKlaHInJqXhjzgG8M/8gYm6xHi0RERFZHgazVsjNyR4/PlcPr7aqCBsbYM7uC+j4zSZsP3NV764RERER5QiDWSveYOHNdlUwe2BDlCpSCBevx6P39O34+N+jrHZAREREFoPBrJULKl8Uy19vnla+66fNZ9F58mYcuhild9eIiIiILCOYnTJlCsqWLQsnJycEBQVh586dWT72119/Vavz0zd5Hj08V0c7fNazFmY8Xw/F3BxxKuKm2gb3m9UnkZSSqnf3iIiIiMw3mJ0zZw5GjBiBcePGYe/evQgICED79u0REZH19qvu7u4IDQ1Na+fPn8/XPhdUrfy9sfL15ni8ZgkkpxoxafUJ9Px+K05FxOjdNSIiIiLzDGYnTpyIgQMHYsCAAahWrRqmTZsGZ2dnzJgxI8vnyGysj49PWvP29s7XPhdkRVwc8F2fOvjmmdpwd7LDwYtRePzbzfh581mkpnIrXCIiIjIvdg/zpOTkZKxfvx6nT59Gnz594ObmhsuXL6sZU1dX12y/TmJiIvbs2YNRo0alHTMYDGjTpg22bduW5fNu3ryJMmXKIDU1FXXr1sWnn36K6tWrZ/rYhIQE1Uyio6PVZVJSkmqZMR3P6n5r0Kl6cdT1a4zRC49g06mr+Ojfo1h5JBSf96gB38KF8vz9OQb64xiYB46D/jgG+uMYWN8YJOXgfWyMRln2k31ySr9Dhw4ICQlRQeKJEydQvnx5DB8+XN2WmdXskgDY19cXW7duRaNGjdKOjxw5Ehs2bMCOHTvueY4EuSdPnkStWrUQFRWFL7/8Ehs3bsSRI0dQqlSpex7/wQcfYPz48fccnzlzppoBpvuT346tETZYdM6AxFQbONoa0aNsKoKKGVVZLyIiIqLcFhcXpyZMJdaTydJcnZmVoLVevXo4cOAAihYtmna8e/fuKl0gr0nQmz7wbdy4MapWrYoffvgBH3300T2Pl1lfyclNPzPr5+eHdu3aZfnhyLeBVatWoW3btrC3t4e1exzAoKtxGLngMPaG3MCs07aIsC+Gj7tWg5erY568J8dAfxwD88Bx0B/HQH8cA+sbg+jbZ9KzI8fB7KZNm9RMqoODQ4bjUo3g0qVLOXotLy8v2NraIjw8PMNxuS25sNkhH2idOnVw6tSpTO93dHRULbPnPWgwsvMYa1HRxwPzXmmM6ZvOYOLKE1hzPBL7LmzDJ91qoGPNEnn2vhwD/XEMzAPHQX8cA/1xDKxnDOxz8B45XgAmeaopKfcW1b948aLKnc0JCYgDAwOxZs2aDK8vt9PPvt6P9OXQoUMoUSLvAirS2Bps8EqLCljyahNULeGOa7GJGPzXXrwxZz+i4pnHRERERPkvx8GsnJ7/+uuvM1QWkAVZUlqrU6dOOe6ApABMnz4dv/32G44dO4bBgwcjNjZWVTcQ/fr1y7BA7MMPP8TKlStx5swZVcrr2WefVXm8L730Uo7fmx6Ov487Fg9tgqGPVYDBBli47xI6fL0Rm05G6t01IiIisjI5TjP46quvVB1YKaN169YtlZwrC7IkZWDWrFk57kCvXr0QGRmJsWPHIiwsDLVr18by5cvTym3JQjOpcGBy/fp1lZsrjy1SpIia2ZW0B+kP5R8HOwPebu+vatO+Ne8Azl6JxXM/70S/RmXwbkd/ODs8VKEMIiIiohzJccQhFQNk8dfs2bNx8OBBNSv74osvom/fvihU6OFKNg0bNky1zEgJsPQmTZqkGpmHwDJFsPS1pvhs2XH8vu28ahtPROKrp2ur+4iIiIjy0kNNn9nZ2anT+0RCZmE/7FoDbat54+15B3HuahyemrZV5de+3qaymsUlIiIiMotg9vfff7/v/ZLjStapWaViWPFGc4xfcgQL9l3C1PWnsS44EhOfDlALxoiIiIhy20PVmb277pgUtpXKBLIJAYNZ6+ZRyB4Te9VGu+reGL3wMI6FRuOJ7zZjRNsqGNS8vKqIQERERJRbcnz+VxZgpW+SMxscHIymTZs+1AIwKpg61CiBFa83V6kHSSlGfL78OJ7+YRvOXYnVu2tERERUgORKMmOlSpXw2Wef3TNrS9atmJsjfnwuEF88WQuujnbYc/46On6zCX9sP48c7qJMRERElKlcW5kji8IuX76cWy9HBYTUIX6qnh+Wv94MjcoXRXxSCsYsOoz+v+xCWNQtvbtHRERE1pYzu2TJkgy3ZYYtNDQU3333HZo0aZKbfaMCpFQRZ/z1UhB+23ZOlfGS8l3tJm1QVRC61i6pgl4iIiKiPA9mu3XrluG2BCHFihVDq1at1IYKRFkxGGwwoEk5VfXgzbn7ceBiFF6fsx8rj4bh42414enioHcXiYiIqKAHs6mpqXnTE7IaFYu74u/BjVXprm/XnMR/h8Kw8+x1fN6zJlpX1XZ+IyIiIsoOVrMnXdjZGvBa60pYNLQJKhV3xZWbCXjxt90YOf8AYm4l6d09IiIiKkgzsyNGjMj2C06cOPFR+kNWpoavB/55tSkmrjqB6ZvOYO7ui9hy6io+71Fd764RERFRQQlm9+3bl60X4yIeehhO9rYY3akqWvsXx1vzD+DCtXg8O2M3WpYwoE1yKuzt9e4hERERWXQwu27durzvCVm9oPJFsWx4c3yy9Bhm7QzB+lADnvtlN6Y9G4ji7k56d4+IiIjMEHNmyazI5goTetTED8/WQSFbI/aG3ECX7zZjb8h1vbtGREREBaGagdi9ezfmzp2LkJAQJCYmZrhvwYIFudU3smKtqhTDmzVTMOeyB05GxOKZH7bjw67V8UyD0np3jYiIiCx5Znb27Nlo3Lgxjh07hoULFyIpKQlHjhzB2rVr4eHhkTe9JKtUrBAwd1AQOlT3QWJKKt5dcAjvLTyExGSWhyMiIqKHDGY//fRTTJo0Cf/88w8cHBzwzTff4Pjx43j66adRujRnzSj30w6+f7Yu3mpXGbK+8K8dIegzfTsiYrgVLhERET1EMHv69Gk8/vjj6roEs7GxsaqKwRtvvIEff/wxL/pIVk5+v4a1qoSf+9eDm5Mddp+/ji6TN2Mf82iJiIisXo6D2SJFiiAmJkZd9/X1xeHDh9X1GzduIC4uLvd7SHRbK39vLBnWVG2yEB6dgF4/bMecXSF6d4uIiIgsIZg1Ba3NmzfHqlWr1PWnnnoKw4cPx8CBA9G7d2+0bt0673pKBKCclwsWDm2C9tW9VR7tO38fwvuLmEdLRERkrbIdzNaqVQtBQUGoWbOmCmLFe++9p3YHCw8PR8+ePfHzzz/nZV+J7uTR9g3Em221PNo/t4eg70/MoyUiIrJG2Q5mN2zYgOrVq2PChAmoWrUq+vfvjy1btuDdd9/FkiVL8NVXX6kUBKL8YDDY4NXWt/NoHe2w69x1PDF5C/NoiYiIrEy2g9lmzZphxowZCA0NxeTJk3Hu3Dm0aNEClStXxueff46wsLC87SlRFnm0i4c1QcXirgiLvqXyaOfuuqB3t4iIiMhcF4C5uLhgwIABaqb2xIkTKuVgypQpqizXE088kTe9JLqP8sVcsXBIY7SrpuXRjvz7IMYsOsw8WiIiIivwSNvZVqxYEaNHj8b7778PNzc3LF26NPd6RpQDbk72mPZsIEa0raxu/7H9vMqjjYxJ0LtrREREZI7B7MaNG/H888/Dx8cHb7/9Nnr06KFyaIn0zKN97a48WqlHu//CDb27RkREROYQzF6+fFntACZ5si1btsSpU6fw7bffquPTp09Hw4YN86qfRNnWuqo3Fg1rggrFXFQe7dPTtjGPloiIqICyy+4DO3bsiNWrV8PLywv9+vXDCy+8gCpVquRt74geUoVirlg0tAlGzD2AVUfDVR7toUtRGNO5GhzsHim7hoiIiCwxmLW3t8f8+fPRuXNn2Nra5m2viHIpj/aHZwMxee0pTFp9QuXRHg2NxtS+deHt7qR394iIiCgXZHuKSmrJdu3alYEsWVwe7fA2t/Noneyw5/x1dJ68GbvOXdO7a0RERJQLeL6VrCaPdsmwpqjs7aoqHPT+cTt+33YORqNR764RERHRI2AwS1ajnJcLFg5pgsdrlUByqhFjFx/BW/MO4lZSit5dIyIioofEYJasioujHb7rXQejO/nDYAP8vfcien6/FReuxendNSIiIrLUYFZ2ECtbtiycnJwQFBSEnTt3Zut5s2fPho2NDbp165bnfaSCQ35nBjWvgD9fDIKniwOOXI7GE99txuaTV/TuGhEREVlaMDtnzhyMGDEC48aNw969exEQEID27dsjIiLivs87d+4c3nrrLTRr1izf+koFS+OKXvjn1aaoVcoD1+OS0G/GDkzbcJp5tERERBZE92B24sSJGDhwIAYMGIBq1aph2rRpcHZ2xowZM7J8TkpKCvr27Yvx48ejfPny+dpfKlh8CxfC3Jcb4anAUkg1Ap8tO44hf+3FzYRkvbtGREREuVlnNi8kJiZiz549GDVqVNoxg8GANm3aYNu2bVk+78MPP0Tx4sXx4osvYtOmTfd9j4SEBNVMoqOj1WVSUpJqmTEdz+p+ynv5OQZSbO6TrlVRo6QbPv7vOJYdDsPJ8BhM7VNbLRqzVvw7MA8cB/1xDPTHMbC+MUjKwfvoGsxeuXJFzbJ6e3tnOC63jx8/nulzNm/ejJ9//hn79+/P1ntMmDBBzeDebeXKlWoG+H5WrVqVrfegvJOfY1AYwNCqwC/BtjgVGavyaJ+tmIqantaddsC/A/PAcdAfx0B/HAPrGYO4uDjLCGZzKiYmBs899xymT5+uttXNDpn1lZzc9DOzfn5+aNeuHdzd3bP8NiCD1bZtW7XzGeU/Pcfg6ZgEvDbnAHafv4Gfgm0xtGV5vPZYBbUBgzXh34F54Djoj2OgP46B9Y1B9O0z6WYfzEpAKjuKhYeHZzgut318fO55/OnTp9XCry5duqQdS01NVZd2dnYIDg5GhQoVMjzH0dFRtbvJQDxoMLLzGMpbeoxBSU97zBrUCJ8sPYZft57DlPVncDQ0Bl/3qgMPZ+v7feDfgXngOOiPY6A/joH1jIF9Dt5D1wVgDg4OCAwMxJo1azIEp3K7UaNG9zze398fhw4dUikGpvbEE0/gscceU9dlxpUoN9jbGvDBE9UxqVcAHO0MWBcciS7fbcax0Ox/UyQiIqK8p3uagaQA9O/fH/Xq1UODBg3w9ddfIzY2VlU3EP369YOvr6/KfZU6tDVq1Mjw/MKFJdMR9xwnyg3d65RCpeJueOXPPQi5FoceU7fi8ydr4YmAknp3jYiIiMwhmO3VqxciIyMxduxYhIWFoXbt2li+fHnaorCQkBBV4YBILzV8PfDPsKZ4bfY+bDp5Ba/N2odDF29gVMeqVpdHS0REZG50D2bFsGHDVMvM+vXr7/vcX3/9NY96RXRHERcH/DqgASauCsaUdacxfdNZRMcnY0KPmgxoiYiIdMQpT6JssjXY4O32/vi6V21I/Dpn9wWM/PsgUmS3BSIiItIFg1miHOpWxxffPFNHBbfz91zE2/MPMKAlIiLSCYNZoofQJaAkvr0d0C7YewlvzWNAS0REpAcGs0QP6fFaJfBd7zqwM9hg4b5LeGPOfiSnaHWPiYiIKH8wmCV6BB1rlsB3feqqgHbJgct4nQEtERFRvmIwS/SIOtTwwdS+dWFva4N/D4Zi+Oz9SGJAS0RElC8YzBLlgnbVffB930A42Bqw9FCoqkXLgJaIiCjvMZglyiVtqnnjh+e0gHbZ4TAMm7kXickMaImIiPISg1miXPSYf3H82C8QDnYGrDgSjqEMaImIiPIUg1miXNaySnFM71cPjnYGrDoajsF/7kFCcore3SIiIiqQGMwS5YEWlYvh5/71VUC75ngEXvljD24lMaAlIiLKbQxmifJI00pemPF8fTjZG7AuOBIvM6AlIiLKdQxmifJQk4paQFvI3hYbTkRi4O+7GdASERHlIgazRHmscQUv/DKgPpwdbLHp5BW89NtuxCcyoCUiIsoNDGaJ8kHD8kXx64AGcHGwxeZTV/Dib7sY0BIREeUCBrNE+aRBOU/89oIW0G49fRUDft2JuMRkvbtFRERk0RjMEuWjemU98fuLDeDqaIftZ67h+V92ITaBAS0REdHDYjBLlM8Cy2gBrZujHXaevYYBv+xCzK0kvbtFRERkkRjMEumgbuki+OOlILg52WHnuWvo8PUmbD55Re9uERERWRwGs0Q6qe1XGDNfaohSRQrh0o14PPvzDoxacJCztERERDnAYJZIRzVLeWDF683Rv1EZdXvWzgtoP2mjqklLRERED8ZglkhnLo52GN+1BmYPaogyRZ1xOeoW+s/YibfnHUBUPGdpiYiI7ofBLJEZ1aJdNrwZXmhSDjY2wLw9F9Fu0gasORaud9eIiIjMFoNZIjPi7GCHsV2qYd7LjVDOywXh0Ql48bfdGDFnP27EJerdPSIiIrPDYJbIDEk9WpmlHdS8PAw2wIJ9l9B20kasOBKmd9eIiIjMCoNZIjPlZG+L0Z2qYv7gxqhQzAWRMQl4+Y89eG3WPlyL5SwtERGRYDBLZAE1aZe+1gyDW1ZQs7RLDlxWubT/HQrVu2tERES6YzBLZCGztO908MfCIU1Q2dsVV24mYshfezH0r724cjNB7+4RERHphsEskQUJ8CuMf15tildbVYStwQZLD4Wi3aSN+OfAZRiNRr27R0RElO8YzBJZGEc7W7zZrgoWD22CqiXcVf7sq7P24ZU/9yAi5pbe3SMiIspXDGaJLFQNXw8V0L7ephLsDDZYcSRczdIu2neJs7RERGQ1GMwSWTAHOwNeb1MZS4Y1RfWS7rgRl4TX5+zHsFn7WJeWiIisglkEs1OmTEHZsmXh5OSEoKAg7Ny5M8vHLliwAPXq1UPhwoXh4uKC2rVr448//sjX/hKZm2ol3bFoaBOMaFtZzdIuPRiKDl9vwuaTV/TuGhERUcEOZufMmYMRI0Zg3Lhx2Lt3LwICAtC+fXtERERk+nhPT0+899572LZtGw4ePIgBAwaotmLFinzvO5E5sbc14LXWlfD34MYo7+WCsOhbePbnHRj/zxHcSkrRu3tEREQFM5idOHEiBg4cqALSatWqYdq0aXB2dsaMGTMyfXzLli3RvXt3VK1aFRUqVMDw4cNRq1YtbN68Od/7TmSuFQ/+fa0pnm1YWt3+Zcs5dJm8GUcuR+ndNSIiolxnBx0lJiZiz549GDVqVNoxg8GANm3aqJnXB5FFLmvXrkVwcDA+//zzTB+TkJCgmkl0dLS6TEpKUi0zpuNZ3U95j2PwaOxtgHGP+6NFpaIYtfAITkbcRLcpW/B664p4sUlZVdbrQTgG5oHjoD+Ogf44BtY3Bkk5eB8bo47Lni9fvgxfX19s3boVjRo1Sjs+cuRIbNiwATt27Mj0eVFRUep5EqTa2tpi6tSpeOGFFzJ97AcffIDx48ffc3zmzJlqBpiooLuZBMw+bcCh69qJmApuRjxbKQWejnr3jIiIKHNxcXHo06ePivnc3d1htjOzD8vNzQ379+/HzZs3sWbNGpVzW758eZWCcDeZ9ZX708/M+vn5oV27dll+OPJtYNWqVWjbti3s7e3z9GehzHEMctdTRiPm772Ej/8LxumYFHx1xAnjOvuja0AJ2NhkPkvLMTAPHAf9cQz0xzGwvjGIvn0mPTt0DWa9vLzUzGp4eHiG43Lbx8cny+dJKkLFihXVdalmcOzYMUyYMCHTYNbR0VG1u8lAPGgwsvMYylscg9zTp2E5NKlUHG/M2Y+9ITfw9t+Hsf7EVXzSvQYKOztk+TyOgXngOOiPY6A/joH1jIF9Dt5D1wVgDg4OCAwMVLOrJqmpqep2+rSDB5HnpM+LJaLMlSnqgrkvN8KbphJeh0LR/uuNLOFFREQWS/dqBpICMH36dPz2229qhnXw4MGIjY1V1Q1Ev379MiwQkxlYmeY+c+aMevxXX32l6sw+++yzOv4URJbDztaAV1tXwoIhWgmv8OgElvAiIiKLpXvObK9evRAZGYmxY8ciLCxMpQ0sX74c3t7e6v6QkBCVVmAige6QIUNw8eJFFCpUCP7+/vjzzz/V6xBR9tUqpZXw+vS/Y/hze4gq4SUztF8/UxvVS3ro3T0iIiLLCGbFsGHDVMvM+vXrM9z++OOPVSOiR+fsYIePu9VEa39vvD3/YFoJrxFtq2BAIz+9u0dERGT+aQZEpL/H/ItjxevN0K6aN5JSjPh8+XE8O2MXrt7Su2dERET3x2CWiJSiro744blA/K9nLbg42GL3+Rv4/KAtJq0+hSs3ucCSiIjME4NZIkojNWefru+HZcObo27pwkhIscHUDWfQ5LO1GL3wEM5eidW7i0RERBkwmCWie5Qu6oyZL9bHgMopqFXKHQnJqZi5IwStvlqPV/7Yg30h1/XuIhERkcJglogyZWuwQe2iRswfFIQ5gxqilX9xyObXy4+EofvUrXh62jasORaO1FTddsQmIiIyj2oGRGTeqQdB5YuqdiI8Bj9uPIPF+y9h57lrqlUs7opBzcqja52ScLSz1bu7RERkZTgzS0TZVtnbDV8+FYBNI1vh5Rbl4eZoh1MRNzHy74No9vk6fL/+NKLik/TuJhERWREGs0SUYz4eThjVsSq2jGqF0Z384e3uiIiYBFXSSxaLfbL0KEKj4vXuJhERWQEGs0T00Nyd7DGoeQU1UysztpW9XXEzIRnTN51VM7Uj5u7H8bBovbtJREQFGHNmieiROdgZ8GRgKfSs64v1wZGYtuE0dpy9hgV7L6nWskoxDGpeHo3KF1U5uERERLmFwSwR5RoJVGU3MWkHLtxQi8WWHQ5VAa60BuU88XG3Gir3loiIKDcwzYCI8kSAX2FM6VsX695qiecaloGjnQE7z15Dp282qdza+MQUvbtIREQFAINZIspTZYq64KNuNbD2rZZoV80byalGVfWg7aQNWHs8XO/uERGRhWMwS0T5wrdwIfzYrx6m96unrl+8Ho8Xft2tdhRj5QMiInpYDGaJKF+1reaNVSOaqzq1dgYbtaNYm6824KdNZ5Cckqp394iIyMIwmCWifOfsYKfq1P77WlPUK1MEsYkp+HjpMXT5bgv2hVzXu3tERGRBGMwSkW78fdwx9+VG+LxnTRR2tsex0Gj0+H4r3lt4CFFx3EmMiIgejMEsEenKYLBBr/qlsWZEC1Wr1mgE/toRgtYT12PRvkswygEiIqIsMJglIrNQ1NVR7SI2e1BDVCzuiis3E/H6nP3o+9MOnI68qXf3iIjITDGYJSKz0rB8Ufz3WjOM7FAFTvYGbD19FR2/3oSJq07gVhJr0xIRUUYMZonILLfHHdKyIla90QKPVSmGxJRUfLvmJNp/vREbT0Tq3T0iIjIjDGaJyGz5eTpjxvP1Me3ZuvBxd8L5q3HoN2Mnhs3ci4joW3p3j4iIzACDWSIyazY2NuhQowRWv9kCLzYtB4MN8O/BULT+agN+23oOKalcIEZEZM0YzBKRRXB1tMOYztWwZFhTBPgVRkxCMsYtOYLuU7fg8KUovbtHREQ6YTBLRBalhq8HFgxujI+61YCbkx0OXozCE99txgdLjiDmFmvTEhFZGwazRGRxbA02eK5hGax5swW61i4JyTT4des5tJm4Af8dCmVtWiIiK8JglogsVnE3J3zzTB388WIDlC3qjPDoBAz5ay8G/LoLIVfj9O4eERHlAwazRGTxmlUqhuWvN8fw1pXgYGvA+uBItJ20AVPWnUJicqre3SMiojzEYJaICgQne1u80bYylr3eDI0rFEVCciq+WBGMx7/dhJ1nr+ndPSIiyiMMZomoQKlQzBV/vRSESb0CUNTFAScjbuLpH7bh7XkHcC02Ue/uERFRLmMwS0QFsjZt9zqlsPbNlugTVFodm7fnIlp/tR5zd1/gAjEiogKEwSwRFVgezvb4tHtN/D24Mfx93HA9Lgkj5x9Erx+240R4jN7dIyKiXMBglogKvMAyRfDPq00xupM/CtnbYue5a+j0zSb8b/lxxCem6N09IiKy9GB2ypQpKFu2LJycnBAUFISdO3dm+djp06ejWbNmKFKkiGpt2rS57+OJiIS9rQGDmldQ2+K2qeqN5FQjpq4/raoerDseoXf3iIjIUoPZOXPmYMSIERg3bhz27t2LgIAAtG/fHhERmf/jsn79evTu3Rvr1q3Dtm3b4Ofnh3bt2uHSpUv53ncisjy+hQvhp/718ONzgSjp4YSL1+NVXVrZRezbNSdx9HI0c2qJiCyI7sHsxIkTMXDgQAwYMADVqlXDtGnT4OzsjBkzZmT6+L/++gtDhgxB7dq14e/vj59++gmpqalYs2ZNvvediCxXu+o+WDWiBQY1L692FJNtcSeuOoFO325C08/XYeziw9h4IhIJyUxDICIyZ3Z6vnliYiL27NmDUaNGpR0zGAwqdUBmXbMjLi4OSUlJ8PT0zPT+hIQE1Uyio6PVpTxHWmZMx7O6n/Iex0B/1jAGDgbg7bYV8XxDP6wLjsSa45HYeuYqLt2Ix+/bzqvm4mCLZpW80KpKMbSo7AVPF4d87aM1jIO54xjoj2NgfWOQlIP3sTHqeD7t8uXL8PX1xdatW9GoUaO04yNHjsSGDRuwY8eOB76GzNKuWLECR44cUTm3d/vggw8wfvz4e47PnDlTzQATEaUn68FORNng8HUbHLlug+gkm7T7bGBEOTegRpFU1PA0wruQrl0lIiqwZLKyT58+iIqKgru7u/nOzD6qzz77DLNnz1Z5tJkFskJmfSUnN/3MrCnPNqsPR74NrFq1Cm3btoW9vX2e9Z+yxjHQH8cASE014vDlaDVjuzY4EsfDYnAmBjgTY4slIUDZos5qxraVfzEEli4MO9vcz9ziOOiPY6A/joH1jUH07TPp2aFrMOvl5QVbW1uEh4dnOC63fXx87vvcL7/8UgWzq1evRq1atbJ8nKOjo2p3k4F40GBk5zGUtzgG+rP2MQgs56XayI5VcfF6HNYej8Cqo+HYfuYqzl2Nw4yt51XzKGSPx6oUQ+uq3mhRpRjcnXL3M7P2cTAHHAP9cQysZwzsc/AeugazDg4OCAwMVIu3unXrpo6ZFnMNGzYsy+f973//wyeffKLSC+rVq5ePPSYia1aqiDP6NSqrWsytJGw6eQWrj4Wr0l6yIcOi/ZdVc7QzoHOtkmr3sbqlC6sdyYiIKG/onmYgKQD9+/dXQWmDBg3w9ddfIzY2VlU3EP369VN5tRMmTFC3P//8c4wdO1blvEpt2rCwMHXc1dVVNSKi/ODmZI9ONUuolpJqxN6Q61h9NByrjoXjTGQs/t57UTXZeaxvUGl0q+OrnkNERAUsmO3VqxciIyNVgCqBqZTcWr58Oby9vdX9ISEhqsKByffff6+qIDz55JMZXkfq1MpiLyKi/CalveqX9VTt3Y7+2HfhBmbuCME/By6rPNsxi49gwrLjeCKgJPoGlUHNUh56d5mIqMDQPZgVklKQVVqBLO5K79y5c/nUKyKinJOUgrqli6g25vFqWLDvIv7aEYJTETcxe9cF1Wr6eqjZ2i4BJeHiaBb/GyYisli6b5pARFRQeTjbY0CTclj1RnPMfbkRutYuCQdbAw5disK7Cw4h6NM1GLPoMI6FZn/VLhERZcQpASKifJitbVDOU7VxXRIxf88FlYYg1RD+2H5eNVko1ieoDDrXKgEne1u9u0xEZDEYzBIR5SPZQWxQ8wp4qWl5bDtzVQW1K46EYW/IDdU+/OcIegaWUmkIFYu76d1dIiKzx2CWiEgHBoMNmlT0Ui0i5hbm7b6IWTtDcPF6PH7Zck41mcntFeiLBO7gSUSUJQazREQ6K+7mhKGPVcTgFhWw8WSkWjC25lg4dp69ppr8r3ramU0I8CuCgFIeqO1XGNVLeqCQA9MRiIgYzBIRmdFsbcsqxVULjYrHnF0XsGT/JZy5EoeQa/GqSbkvUzmwyt5uKrgN8CuMWqU81G37PNhSl4jInDGYJSIyQyU8CuH1NpUxtEU5zF/yH3yqBeFo2E3sv3ADBy7cQERMgqqCIE3KfQkne4OasQ0oVRgBftplmaLO3IGMiAo0BrNERGbO2Q5oWrEoHqvqk3YsLOoWDlzUAtuDF6PU9Zhbydhz/rpqJh6F7NWsrQS2cikbOxRxcdDpJyEiyn0MZomILJCPhxN8PHzQvroW4KamGnHuauztAFcLbo9cjkZUfBI2nbyimrC3tUG7aj7oVd8PTSt6qdQGIiJLxmCWiKgAkKC0fDFX1brXKaWOJSan4kR4TNoM7r6QGzgZcRNLD4Wq5lu4EJ6u54en6pVCycKF9P4RiIgeCoNZIqICysHOgBq+Hqr1DSqjjkmOrSwsW7D3Ii7diMek1SfwzZoTaFG5GHrVL43WVYtzERkRWRQGs0REVqRqCXd88ER1vNvRX23WMHvnBbV5w7rgSNW8XB3Upg296vmpWV4iInPHYJaIyArJlrlda/uqdvZKLObuvqA2brhyMwE/bDijWlA5TzzTwA8da3CLXSIyXwxmiYisXDkvF7zTwR8j2lbGuuMRqtTX+uAI7Dh7TbWxi4+gex1ftWhMSn8REZkTBrNERKRIrmy76j6qyaYN83dfxJzdF9QWu79vO6+alPeSoPaJgJJwc7LXu8tERAxmiYgo800bXm1dSW2zu/X0VczaFYKVR8JUTVtpH/97DI/XKoHeDfxQt3QRbsxARLphMEtERPct+dW0kpdq12ITVRUESUM4FXET8/dcVK1ScVc806A0etTx5YYMRJTvGMwSEVG2eLo44KVm5fFi03LYG3IDs3aG4N+Dl1Xt2o/+PYrPlx9Hh+o+atFYo/JFOVtLRPmCwSwREeWIBKmBZYqoNrZLNSzefxmzd4aoHceWHLismiwqk9zaJwNLwcvVEZbEaDTictQtHLoYhSOXo3AqPAYe8TboaDTq3TUiygSDWSIiemjuTvZ4rmEZ1ST4k9zaxfsuqXJfny07ji9XBKNtNW/0blDaLLfPlcD1wrV4HL4chUOXonD4kgSw0SqlIiNb3JhzEP97KkD9zERkPhjMEhFRrqhZygM1S9XEe52qqvSDWTsvYP+FG1h2OEy1UkUKqc0YnqrnBx8Pp3zvX2qqEeevxamAVbXLchmNqPikex5rZ7BBZW831PT1gKujAb9uPYflR8JxLGwzpvSpq3ZVIyLzwGCWiIhylYujndoaV9rxsGi1y5gsHJMSX1+tOqG20G3lXxzP1C+NllWKwS4Pts+VwPXMlViVJiAzxjLrevRyNGISku95rIOtAVV83G5v/euuAlgJZE0bRSQlJcEj6jTmXnTF+atx6DF1K8Z0ropnG5ZhXjCRGWAwS0REecbf5872ucsOh2LWjgvYee4aVh+LUM3H3QlP1SuFp+v5wc/TWZ32T0hORWxCMmITUnAzIVm12LsuM15PuXP9VjJiE5MRGZOAuMSUe/rjYGdQW/rW9HVHjZISvGqBqxy/nzJuwKIhjTBq0VGsOhqOMYuPYPvZa/isR03W2yXSGYNZIiLKczLL2b1OKdWkrNecXSH4e+8lhEXfwuS1p/DdulPwKGSvgtKklNxZaOVkb0A1Fbh6oLqvh7qsWNxVbQ7xMKR/Pz4XiJ83n1X5wEsPhuLIpShM6VuXO6MR6YjBLBER5SsJKN97vBreal9FzXJKia8tp67iRlzG3FVnB1uVsuDqaAcXR1t1qV3XWvrbro53HiutsLODqqhgm8sLziStQMqT1S1TBK/O3IdzV+PQfepWjOtSDX0alGbaAZEOGMwSEZEuHO1s0blWSdXCom7hZkJSWqDq4mCX64FobpJdz5a+1hRvzTug0iXeW3gYO85cw6c9aqpgmojyT+5n3RMREeWQVDeoWNxNbaMrpa/MOZA1kdnf6f3qYXQnf9Vfqa/7xOTNOBYarXfXiKwKg1kiIqKHJGkFg5pXwNyXG6KEh5OqoNBtyhaVOiGL2Ygo7zGYJSIiekSBZTzx32vN8FiVYqoaw6gFh/DGnP1qQRsR5S0Gs0RERLmgiIsDfu5fX5Uhk7SDRfsvo8t3m1WtXSLKOwxmiYiIcols1/tKiwqYPaihqqF7JlJLO5i76wLTDojyCINZIiKiXFa/rKeqdtCicjHcSkrFyL8P4s25BxCXmDtpB9rmEimIuXXvVrxE1ob1Q4iIiPJAUVdH/PJ8fXy/4TS+WhmMBfsu4eClKHzesyacHey0Xc4S7+xepu16ltmxFLWrWfpd0eR6cqo20xtQygMvNC2HTjVLPPSGEESWTPdgdsqUKfjiiy8QFhaGgIAATJ48GQ0aNMj0sUeOHMHYsWOxZ88enD9/HpMmTcLrr7+e730mIiLKbtrB0Mcqop5ssjBrn9r9rOf323L1PQ5cjMLw2fsx4b/j6Ne4jNq8QcqGEVkLXYPZOXPmYMSIEZg2bRqCgoLw9ddfo3379ggODkbx4sXveXxcXBzKly+Pp556Cm+88YYufSYiIsqpoPJF8d/wZhi94BA2n7qStruZi4O2Y5mzaQczhzs7mjmn393s9vH0u53J7G5CUgpm7byAP7afU1sD/295MCavOYWegb4Y0KQcKhRzzdefMzXViH0XrmPZoTAcuhSF2qULo2ONEmr2mLujUYEMZidOnIiBAwdiwIAB6rYEtUuXLsWMGTPw7rvv3vP4+vXrqyYyu5+IiMhcebk64sd+9XL3RQvZY3ibSnilZXks2X8ZP28+i+NhMfhze4hqUirsxabl0aRi0TwLJpNTUrHz7DUsOxyGFUfCEBGTkHbfjrPX8MOGMyjp4YT2NXxUKkRg6SJqxprI4oPZxMRElS4watSotGMGgwFt2rTBtm25dwomISFBNZPoaK1ESlJSkmqZMR3P6n7KexwD/XEMzAPHQX+WMAaSKdstwAdda3ljx9nr+GXreaw7EYl1wVqr4u2K/o3K4IlaPnC0t33k90tMTsW2M1ex4miE2s73etydz8bF0RatqhRDYJkiKshdf+IKLkfdwi9bzqlWzNUBbasVR/tq3mhQtgjsspHnawljUNAl5fMY5OR9bIw61Qq5fPkyfH19sXXrVjRq1Cjt+MiRI7Fhwwbs2LHjvs8vW7asypd9UM7sBx98gPHjx99zfObMmXB2dn6En4CIiMh8RcQDG8MM2BFhg8RUbSbU1c6IJj5GNPVOhXsO02oTU4BjN2xw4JoNjly3wa2UO7OrLnZG1ChiREBRI6p4GGFnyPi84CgbHLhqg8PXbRCfzeeRdYuLi0OfPn0QFRUFd3d3814Altdk5lfyctPPzPr5+aFdu3ZZfjjybWDVqlVo27Yt7O3t87G3ZMIx0B/HwDxwHPRnyWPwPICo+CTM3XMRf2y/gNCoW1hx0QZrQ23RuVYJDGhUBlVLuGX5fKmcsD44Us3AbjgRifik1LT7HmaGNbMZ3R2RNtgRCZU7LDO6Hap7o1mlonBKN4Os9xhIvyV9QvKSw6PvXIZH38KVm4kIKlcELzcvD8cCHI0n5fMYmM6kZ4duwayXlxdsbW0RHh6e4bjc9vHxybX3cXR0VO1uMhAPGozsPIbyFsdAfxwD88Bx0J+ljoGXvT2GPFYZg5pXxPIjYSqvdl/IDSzcd1m1RuWL4sWm5dDKv7jKZb0Rl4hVR8Ox/HAYNp26ogI5E9/ChdChhg861vBB3YfIfZWPr031kqqlz7WVfkXGJGDJwVDVZIHcY1WKq/d6zL84HG9/7nkxBlKrV4JSCfTDTC36zqUpYL2fneeuY/mRCHz5VAAC/AqjILPPp7+DnLyHbsGsg4MDAgMDsWbNGnTr1k0dS01NVbeHDRumV7eIiIgKJJk57VyrpGp7Q65jxuazKpCUmVJpZYs6o1QRZ2w/czWthq0o7+VyO4AtgRq+7rm2kEz607iil2rjn6iu+vTfIW0R2aUb8Vh6KFQ1BzsDmlUsCrsYA46vOgmjjUFtGpGSakSK0agqKEh3TddTbt9OTfeYtMenasdvJaWoIFVmV2X2OTscbA3w8XBSO7t5ezihhIcTvN2dYG9rg2/XnMTJiJvo8f1WNUMri/Ic7R49N5myR9c0Azn9379/f9SrV0/VlpXSXLGxsWnVDfr166fyaidMmJC2aOzo0aNp1y9duoT9+/fD1dUVFStW1PNHISIishgyq1q3TxEVNP6+7Rxm7QjBuatxqgl/HzcVvEoQW9nbNc/LaskMb72ynqqN6VwVBy9GaTO2h0NVn9Ycj9SWuV06myfv7+5kpwWqHoXg4+6oAlZ13cNRBawlPAqhiLN9lp9Dl1olMW7JESw5cBlT159WM9vWMEtrLnQNZnv16oXIyEi1EYJsmlC7dm0sX74c3t7e6v6QkBBV4SD9orE6deqk3f7yyy9Va9GiBdavX6/Lz0BERGSpJG1gVMeqeK1VJfxz4LKapWxd1RvlvFx065MEjBIESnunQxVVamzZocvYd+QkypYrC3tbW0h6rgTAtjY2MEi7fV2Oy/NtTfepS+2xhnTHZba3eFrQ6qRq9j6KIi4O+LZ3HVV67P1Fh8xiljYhOQW7zl5HqSKFUFbH8cwPui8Ak5SCrNIK7g5QpYKBTsUXiIiICizZhOGZBqVhbiQwrVrCHRW9CuG/+GB06uRv1nnLMpPdoJynmqX95/Ys7epj2ixtrVL5M0sbGZOAv3acx5/bz6fl+lYo5oI2Vb3VF5W6pQtna7GeJdE9mCUiIiIqKDxdHDC5dx08XtMH7y86jBPhN9F96la80qI8Xmudd7O0Ry9HY8aWs2rzjERJDr7dl+j4JJyOjMXpyDP4YeMZFHa2V4vrWlctjuaVi8HdyXy/HGQXg1kiIiKiXNahRgk0KFc0bZZ2yro7ubS5NUsri9rWHo9Qi/lkEZ9Jbb/CqkKFzBTHJ6VgQ3Ak1hwLVxto3IhLwsJ9l1SzM9ggqLwnWvt7q5nb0kUts/4+g1kiIiIiC5qlldzm+bsv4Jet53D+9qI9yQeWkmkvNC2nFviZ2Nsa0CWgpGpSDm3P+etYc1zq/IbjTGQstpy6qtqH/x5Vi/0kFaFN1eKo7VdEvaYlYDBLRERElA+ztGMXH8a/B0PVLO3qoxH44qlaOZqlvXAtTlWfmL3rAmJuJadVYugdVBr9G5VFycKF7vt8yZUNKl9UtdGdquLslVg1YyuB7a5z11WwLe379adVIC7pCBLYNqtcDI5mnGbLYJaIiIgoj0lw+F2funi8ZqiapQ0Oj8nWLK0sfN99XqsLLDV4TSWApf7vgKbl0LOu70NXY5CqFS81K69aVFwS1p/QdmZbHxyBa7GJ+HvvRdWkxm6DckXgnWyDoNhE+BQ2rzxbBrNERERE+aRjzRJqZvTuWVrJpa1ZyiPtcbLz2n+HQtWiLqm7a9KskhdeaFIOLSoXy/EObPfj4WyPrrV9VUtKScWuc9ew5liEmrmVWr+bT0lOri2evXELPoXNq9QXg1kiIiIiHWZpO9UMxZjbs7Tdpm7B4BYV8FyjMpi/56JKJ5AdyoTUxe1RxxcDmpRDFR+3PO+fvezOVsFLtfcfr6qqIaw8chnLdgajesm8f/+cYjBLREREpAPZZCGonCfGLjmCpQdD8d26U6qZFHNzRL+GZdAnqDSKujrqVuu3YnFXlClSDr7Rx/J8N7iHwWCWiIiISCcSpE65nUsrs7RXYxNRw9ddldZ6vGZJNStL98dgloiIiMgMZmmbVPTC5Rvx8PdxM8sZUHPFYJaIiIjIDHgUsleNcoZz10RERERksRjMEhEREZHFYjBLRERERBaLwSwRERERWSwGs0RERERksRjMEhEREZHFYjBLRERERBaLwSwRERERWSwGs0RERERksRjMEhEREZHFYjBLRERERBaLwSwRERERWSwGs0RERERksRjMEhEREZHFsoOVMRqN6jI6OjrLxyQlJSEuLk49xt7ePh97RyYcA/1xDMwDx0F/HAP9cQysbwyib8dpprjtfqwumI2JiVGXfn5+eneFiIiIiB4Qt3l4eNzvIbAxZifkLUBSU1Nx+fJluLm5wcbGJstvAxLsXrhwAe7u7vneR+IYmAOOgXngOOiPY6A/joH1jYHRaFSBbMmSJWEw3D8r1upmZuUDKVWqVLYeK4PFPxp9cQz0xzEwDxwH/XEM9McxsK4x8HjAjKwJF4ARERERkcViMEtEREREFovBbCYcHR0xbtw4dUn64Bjoj2NgHjgO+uMY6I9joD9HMx4Dq1sARkREREQFB2dmiYiIiMhiMZglIiIiIovFYJaIiIiILBaDWSIiIiKyWAxmMzFlyhSULVsWTk5OCAoKws6dO/XuktX44IMP1M5s6Zu/v7/e3SrQNm7ciC5duqhdVuTzXrRoUYb7ZY3o2LFjUaJECRQqVAht2rTByZMndeuvNY7B888/f8/fRYcOHXTrb0E0YcIE1K9fX+0OWbx4cXTr1g3BwcEZHnPr1i0MHToURYsWhaurK3r27Inw8HDd+myNY9CyZct7/hZeeeUV3fpc0Hz//feoVatW2sYIjRo1wrJly8z+b4DB7F3mzJmDESNGqPITe/fuRUBAANq3b4+IiAi9u2Y1qlevjtDQ0LS2efNmvbtUoMXGxqrfc/kSl5n//e9/+PbbbzFt2jTs2LEDLi4u6m9C/qdG+TMGQoLX9H8Xs2bNytc+FnQbNmxQ/0hv374dq1atQlJSEtq1a6fGxuSNN97AP//8g3nz5qnHy9boPXr00LXf1jYGYuDAgRn+FuT/UZQ7ZIfUzz77DHv27MHu3bvRqlUrdO3aFUeOHDHvvwEpzUV3NGjQwDh06NC02ykpKcaSJUsaJ0yYoGu/rMW4ceOMAQEBenfDasn/EhYuXJh2OzU11ejj42P84osv0o7duHHD6OjoaJw1a5ZOvbSuMRD9+/c3du3aVbc+WaOIiAg1Fhs2bEj7vbe3tzfOmzcv7THHjh1Tj9m2bZuOPbWeMRAtWrQwDh8+XNd+WZsiRYoYf/rpJ7P+G+DMbDqJiYnq24icRjUxGAzq9rZt23TtmzWRU9hyurV8+fLo27cvQkJC9O6S1Tp79izCwsIy/E3IXtmSfsO/ify1fv16deq1SpUqGDx4MK5evap3lwq0qKgodenp6aku5d8GmSlM/7cgKVClS5fm30I+jYHJX3/9BS8vL9SoUQOjRo1CXFycTj0s2FJSUjB79mw1My7pBub8N2Cn67ubmStXrqjB8/b2znBcbh8/fly3flkTCZJ+/fVX9Q+2nD4aP348mjVrhsOHD6s8KspfEsiKzP4mTPdR3pMUAzmVV65cOZw+fRqjR49Gx44d1T8gtra2enevwElNTcXrr7+OJk2aqIBJyO+7g4MDChcunOGx/FvIvzEQffr0QZkyZdSEx8GDB/HOO++ovNoFCxbo2t+C5NChQyp4lVQyyYtduHAhqlWrhv3795vt3wCDWTIr8g+0iSShS3Ar/+OaO3cuXnzxRV37RqSXZ555Ju16zZo11d9GhQoV1Gxt69atde1bQSR5m/IFmvn65jcGgwYNyvC3IAtT5W9AvuTJ3wQ9OplMksBVZsbnz5+P/v37q/xYc8Y0g3TktIXMcty9Mk9u+/j46NYvaybfACtXroxTp07p3RWrZPq959+EeZEUHPn/Ff8uct+wYcPw77//Yt26dWoxjIn8vksq2o0bNzI8nn8L+TcGmZEJD8G/hdwjs68VK1ZEYGCgqjAhi1O/+eYbs/4bYDB71wDK4K1ZsybDqQ65LVPulP9u3rypvnHLt2/Kf3JaW/4nlf5vIjo6WlU14N+Efi5evKhyZvl3kXtk7Z0EUXJKde3atep3Pz35t8He3j7D34Kc3pacfv4t5M8YZEZmEAX/FvKOxEEJCQlm/TfANIO7SFkumVKvV68eGjRogK+//lolPw8YMEDvrlmFt956S9XblNQCKfkhJdJktrx37956d61Af2FIP6shi77kHwhZdCGJ/ZK39vHHH6NSpUrqH5cxY8aofDWpAUl5PwbSJHdc6jnKFwv5cjdy5Eg1cyIl0ij3TmvPnDkTixcvVvn5phxAWfAo9ZXlUlKd5N8IGROpwfnqq6+qf8QbNmyod/etYgzkd1/u79Spk6pzKjmzUiqqefPmKvWGHp0sqJN0P/l/f0xMjPq8JZ1pxYoV5v03oGstBTM1efJkY+nSpY0ODg6qVNf27dv17pLV6NWrl7FEiRLqs/f19VW3T506pXe3CrR169ap0ip3NykHZSrPNWbMGKO3t7cqydW6dWtjcHCw3t22mjGIi4sztmvXzlisWDFVFqdMmTLGgQMHGsPCwvTudoGS2ecv7Zdffkl7THx8vHHIkCGqVJGzs7Oxe/fuxtDQUF37bU1jEBISYmzevLnR09NT/b+oYsWKxrffftsYFRWld9cLjBdeeEH9P0b+DZb/58j/71euXGn2fwM28h99w2kiIiIioofDnFkiIiIislgMZomIiIjIYjGYJSIiIiKLxWCWiIiIiCwWg1kiIiIislgMZomIiIjIYjGYJSIiIiKLxWCWiIiIiCwWg1kiIitlY2ODRYsW6d0NIqJHwmCWiEgHzz//vAom724dOnTQu2tERBbFTu8OEBFZKwlcf/nllwzHHB0ddesPEZEl4swsEZFOJHD18fHJ0IoUKaLuk1na77//Hh07dkShQoVQvnx5zJ8/P8PzDx06hFatWqn7ixYtikGDBuHmzZsZHjNjxgxUr15dvVeJEiUwbNiwDPdfuXIF3bt3h7OzMypVqoQlS5bkw09ORJR7GMwSEZmpMWPGoGfPnjhw4AD69u2LZ555BseOHVP3xcbGon379ir43bVrF+bNm4fVq1dnCFYlGB46dKgKciXwlUC1YsWKGd5j/PjxePrpp3Hw4EF06tRJvc+1a9fy/WclInpYNkaj0fjQzyYioofOmf3zzz/h5OSU4fjo0aNVk5nZV155RQWkJg0bNkTdunUxdepUTJ8+He+88w4uXLgAFxcXdf9///2HLl264PLly/D29oavry8GDBiAjz/+ONM+yHu8//77+Oijj9ICZFdXVyxbtoy5u0RkMZgzS0Skk8ceeyxDsCo8PT3Trjdq1CjDfXJ7//796rrM0AYEBKQFsqJJkyZITU1FcHCwClQlqG3duvV9+1CrVq206/Ja7u7uiIiIeOSfjYgovzCYJSLSiQSPd5/2zy2SR5sd9vb2GW5LECwBMRGRpWDOLBGRmdq+ffs9t6tWraquy6Xk0kpqgMmWLVtgMBhQpUoVuLm5oWzZslizZk2+95uIKD9xZpaISCcJCQkICwvLcMzOzg5eXl7quizqqlevHpo2bYq//voLO3fuxM8//6zuk4Va48aNQ//+/fHBBx8gMjISr776Kp577jmVLyvkuOTdFi9eXFVFiImJUQGvPI6IqKBgMEtEpJPly5erclnpyazq8ePH0yoNzJ49G0OGDFGPmzVrFqpVq6buk1JaK1aswPDhw1G/fn11WyofTJw4Me21JNC9desWJk2ahLfeeksFyU8++WQ+/5RERHmL1QyIiMyQ5K4uXLgQ3bp107srRERmjTmzRERERGSxGMwSERERkcViziwRkRliBhgRUfZwZpaIiIiILBaDWSIiIiKyWAxmiYiIiMhiMZglIiIiIovFYJaIiIiILBaDWSIiIiKyWAxmiYiIiMhiMZglIiIiIliq/wNkMpqtSva65AAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# train_classifier \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -----------------------------\n",
    "# Config\n",
    "# -----------------------------\n",
    "EMB_PATH = \"embeddings_split.pt\"\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS     = 30\n",
    "LR         = 1e-3\n",
    "MODEL_SAVE = \"mlp_classifier.pt\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# -----------------------------\n",
    "# Load embeddings\n",
    "# -----------------------------\n",
    "data = torch.load(EMB_PATH)\n",
    "X_train, y_train = data[\"X_train\"], data[\"y_train\"]\n",
    "X_val, y_val     = data[\"X_val\"], data[\"y_val\"]\n",
    "\n",
    "# -----------------------------\n",
    "# Classifier\n",
    "# -----------------------------\n",
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=128, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "clf = MLPClassifier(input_dim).to(device)\n",
    "\n",
    "# -----------------------------\n",
    "# Training\n",
    "# -------------- --\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(clf.parameters(), lr=LR)\n",
    "\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "train_losses, val_accuracies = [], []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    clf.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for xb, yb in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\"):\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = clf(xb)\n",
    "        loss = criterion(out, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    train_losses.append(avg_loss)\n",
    "\n",
    "    # Validation check\n",
    "    clf.eval()\n",
    "    with torch.no_grad():\n",
    "        preds = clf(X_val.to(device)).argmax(dim=1).cpu()\n",
    "    \n",
    "    acc = accuracy_score(y_val, preds)\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(y_val, preds, average=\"binary\", zero_division=0)\n",
    "    val_accuracies.append(acc)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} | Loss: {avg_loss:.4f} | \"\n",
    "          f\"Val Acc: {acc:.4f}, Prec: {prec:.4f}, Recall: {rec:.4f}, F1: {f1:.4f}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Save Classifier\n",
    "# ---------------- -\n",
    "torch.save(clf.state_dict(), MODEL_SAVE)\n",
    "print(f\"Classifier saved to {MODEL_SAVE}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Plot\n",
    "# ------------ \n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(range(1, EPOCHS+1), train_losses, label=\"Train Loss\")\n",
    "plt.plot(range(1, EPOCHS+1), val_accuracies, label=\"Val Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.title(\"Training Progress\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "75405bd4-942e-4130-82f2-00b28fb5c964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Test Evaluation (Unseen Test Set):\n",
      "Accuracy : 0.8626716604244694\n",
      "Precision: 0.8307692307692308\n",
      "Recall   : 0.8804347826086957\n",
      "F1       : 0.8548812664907651\n",
      "Confusion Matrix:\n",
      " [[367  66]\n",
      " [ 44 324]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_19728\\1201666541.py:16: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(EMB_PATH)\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_19728\\1201666541.py:34: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  clf.load_state_dict(torch.load(MODEL_SAVE))\n"
     ]
    }
   ],
   "source": [
    "# test_classifier \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "\n",
    "# -----------------------------\n",
    "# Config\n",
    "# -----------------------------\n",
    "EMB_PATH = \"embeddings_split.pt\"\n",
    "MODEL_SAVE = \"mlp_classifier.pt\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# -----------------------------\n",
    "# Load embeddings\n",
    "# -----------------------------\n",
    "data = torch.load(EMB_PATH)\n",
    "X_test, y_test = data[\"X_test\"], data[\"y_test\"]\n",
    "\n",
    "# -----------------------------\n",
    "# Define MLP (must match training)\n",
    "# -----------------------------\n",
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=128, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))\n",
    "\n",
    "input_dim = X_test.shape[1]\n",
    "clf = MLPClassifier(input_dim).to(device)\n",
    "clf.load_state_dict(torch.load(MODEL_SAVE))\n",
    "clf.eval()\n",
    "\n",
    "# -----------------------------\n",
    "# Final Evaluation\n",
    "# -----------------------------\n",
    "with torch.no_grad():\n",
    "    preds = clf(X_test.to(device)).argmax(dim=1).cpu()\n",
    "\n",
    "acc = accuracy_score(y_test, preds)\n",
    "prec, rec, f1, _ = precision_recall_fscore_support(y_test, preds, average=\"binary\", zero_division=0)\n",
    "cm = confusion_matrix(y_test, preds)\n",
    "\n",
    "print(\"\\nFinal Test Evaluation (Unseen Test Set):\")\n",
    "print(\"Accuracy :\", acc)\n",
    "print(\"Precision:\", prec)\n",
    "print(\"Recall   :\", rec)\n",
    "print(\"F1       :\", f1)\n",
    "print(\"Confusion Matrix:\\n\", cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fcb6b7-9148-47fa-a311-94d47fc88ae1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
